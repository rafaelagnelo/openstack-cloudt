{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Computa\u00e7\u00e3o em Nuvem","text":"Edi\u00e7\u00e3o <p>2025.1</p>"},{"location":"#kit-t","title":"KIT-T","text":"<p>Pedro Henrique Vidal</p> <p>Rafael Agnelo</p>"},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Roteiro 1 - Data 23/02/2025</li> <li> Roteiro 2</li> <li> Roteiro 3</li> <li> Roteiro 4</li> <li> Projeto</li> </ul>"},{"location":"#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"projeto/main/","title":"Roteiro Final - Projeto 2025.1: API RESTful + Docker + AWS Lightsail","text":""},{"location":"projeto/main/#link-do-projeto-github-projeto-cloud-20251","title":"Link do Projeto: GitHub - Projeto Cloud 2025.1","text":""},{"location":"projeto/main/#integrantes","title":"Integrantes:","text":"<ul> <li>Rafael Agnelo </li> <li>Pedro Henrique Vidal</li> </ul>"},{"location":"projeto/main/#1-objetivo-do-projeto","title":"1. Objetivo do Projeto","text":"<p>O objetivo do nosso projeto foi construir uma aplica\u00e7\u00e3o completa baseada em uma API RESTful com FastAPI, capaz de:</p> <ul> <li>Cadastrar e autenticar usu\u00e1rios</li> <li>Proteger os endpoints com autentica\u00e7\u00e3o JWT</li> <li>Fazer scraping de dados reais (atualizados diariamente)</li> <li>Containerizar a aplica\u00e7\u00e3o com Docker</li> <li>Implantar a solu\u00e7\u00e3o na nuvem utilizando AWS Lightsail</li> </ul> <p>Al\u00e9m disso, nos preocupamos com boas pr\u00e1ticas como seguran\u00e7a, estrutura de pastas, documenta\u00e7\u00e3o com MkDocs e monitoramento de custos.</p> <p>\ud83d\udcf8 Imagem conceitual ou diagrama geral do projeto </p>"},{"location":"projeto/main/#2-etapa-1-construcao-e-dockerizacao-da-api","title":"2. Etapa 1 \u2013 Constru\u00e7\u00e3o e Dockeriza\u00e7\u00e3o da API","text":""},{"location":"projeto/main/#21-desenvolvimento-da-api-com-fastapi","title":"2.1 Desenvolvimento da API com FastAPI","text":"<p>Criamos uma aplica\u00e7\u00e3o em Python utilizando FastAPI como framework principal. A estrutura inicial foi organizada da seguinte forma:</p> <pre><code>api/\n  \u251c\u2500\u2500 app/\n  \u2502   \u251c\u2500\u2500 main.py\n  \u2502   \u251c\u2500\u2500 auth.py\n  \u2502   \u251c\u2500\u2500 models.py\n  \u2502   \u251c\u2500\u2500 database.py\n  \u2502   \u2514\u2500\u2500 scraping.py\n  \u251c\u2500\u2500 Dockerfile\n  \u251c\u2500\u2500 requirements.txt\ncompose.yaml\n.env\n</code></pre>"},{"location":"projeto/main/#22-endpoints-implementados","title":"2.2 Endpoints Implementados","text":"<p>1. POST /registrar</p> <ul> <li>Recebe nome, email e senha do usu\u00e1rio</li> <li>Valida se o email j\u00e1 est\u00e1 cadastrado</li> <li>Salva o usu\u00e1rio com a senha criptografada (bcrypt)</li> <li>Retorna um token JWT</li> </ul> <p>2. POST /login</p> <ul> <li>Verifica credenciais (email e senha)</li> <li>Gera e retorna um novo token JWT se forem v\u00e1lidas</li> </ul> <p>3. GET /consultar</p> <ul> <li>Protegido por JWT</li> <li>Realiza scraping de dados atualizados (utilizamos a API da AwesomeAPI para capturar cota\u00e7\u00f5es do BTC em BRL)</li> <li>Retorna os dados em JSON</li> </ul> <p>4. GET /health-check</p> <ul> <li>Endpoint simples para verificar se a aplica\u00e7\u00e3o est\u00e1 ativa</li> <li>Usado pela AWS Lightsail para monitoramento</li> </ul> <p>Endpoint Registrar:</p> <p></p> <p>Endpoint Consulta:</p> <p></p> <p>Endpoint Health Check: </p>"},{"location":"projeto/main/#23-containerizacao-com-docker","title":"2.3 Containeriza\u00e7\u00e3o com Docker","text":"<p>Criamos um <code>Dockerfile</code> para empacotar nossa aplica\u00e7\u00e3o FastAPI e adicionamos o <code>compose.yaml</code> com dois servi\u00e7os:</p> <ul> <li>app: a aplica\u00e7\u00e3o FastAPI</li> <li>db: PostgreSQL (vers\u00e3o 17)</li> </ul> <pre><code>services:\n  db:\n    image: postgres:17\n    environment:\n      POSTGRES_DB: ${POSTGRES_DB:-projeto}\n      POSTGRES_USER: ${POSTGRES_USER:-projeto}\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-projeto}\n    ports:\n      - \"5432:5432\"\n\n  app:\n    image: rafaelagnelo/projetocloudtrafaelph:latest\n    ports:\n      - \"8080:8080\"\n    environment:\n      DATABASE_URL: \"postgresql://${POSTGRES_USER:-projeto}:${POSTGRES_PASSWORD:-projeto}@db/${POSTGRES_DB:-projeto}\"\n      JWT_SECRET: \"troque-esta-chave\"\n    depends_on:\n      db:\n        condition: service_healthy\n</code></pre> <p>Executamos tudo com o comando:</p> <pre><code>docker compose up --build -d\n</code></pre> <p>Publicamos nossa imagem no Docker Hub:</p> <pre><code>docker build -t rafaelagnelo/projetocloudtrafaelph:latest .\ndocker push rafaelagnelo/projetocloudtrafaelph:latest\n</code></pre> <p>\ud83c\udfa5 V\u00eddeo de execu\u00e7\u00e3o da Etapa 1 local (docker compose): https://youtu.be/hywXJr8h0Ig</p>"},{"location":"projeto/main/#3-etapa-2-deploy-na-aws-lightsail","title":"3. Etapa 2 \u2013 Deploy na AWS Lightsail","text":""},{"location":"projeto/main/#31-criacao-do-container-no-lightsail","title":"3.1 Cria\u00e7\u00e3o do Container no Lightsail","text":"<p>Acessamos o painel do AWS Lightsail e criamos um novo servi\u00e7o de container:</p> <ul> <li>Nome: <code>fastapi-service</code></li> <li>Power: Micro</li> <li>N\u00famero de inst\u00e2ncias: 1</li> <li>Imagem: <code>rafaelagnelo/projetocloudtrafaelph:latest</code></li> </ul> <p>\ud83d\udcf8 Painel do container na AWS Lightsail: </p>"},{"location":"projeto/main/#32-banco-de-dados-no-lightsail","title":"3.2 Banco de Dados no Lightsail","text":"<p>Criamos um banco PostgreSQL gerenciado:</p> <ul> <li>Nome: <code>fastapi-db</code></li> <li>Usu\u00e1rio: <code>admin</code></li> <li>Senha: [armazenada em ambiente]</li> <li>Modo p\u00fablico ativado</li> <li>Endpoint copiado e usado na vari\u00e1vel <code>DATABASE_URL</code></li> </ul> <p>\ud83d\udcf8 Painel do banco de dados na AWS Lightsail: </p>"},{"location":"projeto/main/#33-variaveis-de-ambiente","title":"3.3 Vari\u00e1veis de Ambiente","text":"<p>As credenciais e chaves JWT foram definidas via painel do Lightsail em cada servi\u00e7o:</p> <pre><code>DATABASE_URL=postgresql://admin:senha@endpoint.aws.com:5432/fastapi-db\nJWT_SECRET=minha-chave-jwt\n</code></pre> <p>\ud83d\udcf8 Se\u00e7\u00e3o de environment variables no container service: </p>"},{"location":"projeto/main/#34-testes-de-implantacao","title":"3.4 Testes de Implanta\u00e7\u00e3o","text":"<p>Testamos os endpoints pelo pr\u00f3prio link fornecido pela AWS Lightsail. Verificamos:</p> <ul> <li>Cria\u00e7\u00e3o de usu\u00e1rios</li> <li>Login e gera\u00e7\u00e3o de token</li> <li>Acesso ao <code>/consultar</code> com scraping de dados reais</li> <li>Funcionamento do <code>/health-check</code></li> </ul> <p>O dom\u00ednio publico foi configurado para apontar para o servi\u00e7o de container, permitindo acesso externo.</p>"},{"location":"projeto/main/#link-do-dominio-de-acesso-public-domain-fastapi-service","title":"Link do dom\u00ednio de acesso: Public Domain FastAPI Service","text":""},{"location":"projeto/main/#prints-dos-testes-realizados","title":"Prints dos testes realizados:","text":"<p>Endpoint registrar: </p> <p>Resposta do endpoint registrar: </p> <p>Endpoint login/consulta: </p> <p>Resposta do endpoint login/consulta: </p> <p>Resposta do endpoint login/consulta: </p> <p>Sa\u00edda de resposta da API no terminal: </p>"},{"location":"projeto/main/#35-custos","title":"3.5 Custos","text":"<p>Para controle dos custos, acessamos o painel de billing da AWS. O custo di\u00e1rio para 1 inst\u00e2ncia Micro + banco ficou dentro da meta (R$ 0,07 por hora). Tamb\u00e9m projetamos os custos para:</p> <ul> <li>1 inst\u00e2ncia: ~$5/m\u00eas</li> <li>5 inst\u00e2ncias: ~$25/m\u00eas</li> <li>10 inst\u00e2ncias: ~$50/m\u00eas (limite do projeto)</li> </ul> <p>\ud83c\udfa5 V\u00eddeo da Etapa 2 (aplica\u00e7\u00e3o rodando na AWS com banco em cloud): https://youtu.be/IRWKyjIClms</p>"},{"location":"projeto/main/#5-conclusao","title":"5. Conclus\u00e3o","text":"<p>O projeto foi essencial para consolidarmos conhecimentos em FastAPI, autentica\u00e7\u00e3o JWT, web scraping, Docker, deploy em cloud e boas pr\u00e1ticas de seguran\u00e7a e documenta\u00e7\u00e3o. Sa\u00edmos desse processo mais preparados para lidar com infraestrutura real e desafios pr\u00e1ticos do dia a dia em produ\u00e7\u00e3o.</p>"},{"location":"roteiro1/main/","title":"Roteiro 1 - MAAS","text":""},{"location":"roteiro1/main/#1-objetivo","title":"1. Objetivo","text":"<p>O presente relat\u00f3rio tem como alvo abstrair os seguintes conhecimentos:</p> <p>\u2022   Entendimento de conceitos b\u00e1sicos sobre gerenciamento de hardware (Bare Metal) e MaaS (Metal as a Service).</p> <p>\u2022   Entendimento de conceitos b\u00e1sicos sobre redes de computadores.</p>"},{"location":"roteiro1/main/#2-infraestrutura","title":"2. Infraestrutura","text":"<p>O kit utilizado para esta atividade foi composto pelos seguintes itens:</p> <ul> <li> <p>1 NUC (main) com 10 GB de RAM e 1 SSD de 120 GB.</p> </li> <li> <p>1 NUC (server1) com 12 GB de RAM e 1 SSD de 120 GB.</p> </li> <li> <p>1 NUC (server2) com 16 GB de RAM e 2 SSDs de 120 GB.</p> </li> <li> <p>3 NUCs (server3, server4, server5) com 32 GB de RAM e 2 SSDs de 120 GB.</p> </li> <li> <p>1 Switch D-Link DSG-1210-28 com 28 portas.</p> </li> <li> <p>1 Roteador TP-Link TL-R470T+.</p> </li> </ul> <p>Para a reliza\u00e7\u00e3o desse roteiro contamos com um ponto de rede (cabo) pr\u00f3prio conectado \u00e0 rede do Insper e um IP de entrada configurado diretamente no roteador.</p>"},{"location":"roteiro1/main/#21-instalacao-do-ubuntu-server-na-maquina-principal-main","title":"2.1 Instala\u00e7\u00e3o do Ubuntu Server na m\u00e1quina principal (main)","text":"<p>A primeira etapa da configura\u00e7\u00e3o envolveu a instala\u00e7\u00e3o do sistema operacional Ubuntu Server 22.04 LTS na m\u00e1quina principal (NUC main). O procedimento foi realizado via pendrive boot\u00e1vel, configurado previamente com a imagem ISO do Ubuntu.</p> <p>As defini\u00e7\u00f5es aplicadas durante a instala\u00e7\u00e3o foram: Instala\u00e7\u00e3o do Ubuntu:</p> <p>Hostname: main</p> <p>Usu\u00e1rio: cloud</p> <p>Senha: cloudt</p>"},{"location":"roteiro1/main/#22-instalacao-do-maas","title":"2.2 Instala\u00e7\u00e3o do MAAS","text":"<p>Com o sistema operacional instalado, foi realizada a instala\u00e7\u00e3o do MAAS (Metal as a Service) na vers\u00e3o est\u00e1vel 3.5.3. Antes disso, foram feitos testes de conectividade com os comandos ping para verificar roteamento e resolu\u00e7\u00e3o de DNS.</p> <p>Com a conectividade confirmada, os seguintes comandos foram executados:</p> sudo apt update &amp;&amp; sudo apt upgrade -ysudo snap install maas --channel=3.5/stablesudo snap install maas-test-db"},{"location":"roteiro1/main/#23-inicializacao-do-maas-e-criacao-do-administrador","title":"2.3 Inicializa\u00e7\u00e3o do MAAS e cria\u00e7\u00e3o do administrador","text":"<p>A inicializa\u00e7\u00e3o do MAAS foi feita com os seguintes comandos:</p> <p><pre><code>$ sudo maas init region+rack --maas-url http://172.16.0.3:5240/MAAS --database-uri maas-test-db:///\n$ sudo maas createadmin\n</code></pre> Durante a cria\u00e7\u00e3o do administrador, utilizamos o login cloud, a senha padr\u00e3o da disciplina e deixamos o campo de chave SSH vazio.</p> <p>Depois, geramos um par de chaves SSH com senha vazia:</p> <pre><code>$ ssh-keygen -t rsa\n$ cat ./.ssh/id_rsa.pub\n</code></pre> <p>A chave p\u00fablica gerada foi copiada e registrada no dashboard do MAAS.</p>"},{"location":"roteiro1/main/#24-configuracao-inicial-do-dashboard-do-maas-e-configuracao-do-dhcp","title":"2.4 Configura\u00e7\u00e3o inicial do Dashboard do MAAS e configura\u00e7\u00e3o do DHCP","text":"<p>Acessamos o dashboard do MAAS via navegador em http://172.16.0.3:5240/MAAS.</p> <p>As configura\u00e7\u00f5es aplicadas nessa etapa inclu\u00edram: Upload da chave SSH gerada; Defini\u00e7\u00e3o do DNS Forwarder para 172.20.129.131; Importa\u00e7\u00e3o das imagens do Ubuntu 22.04 LTS e 20.04 LTS; Atualiza\u00e7\u00e3o do par\u00e2metro global net.ifnames=0 em Settings &gt; General.</p> <p>Tamb\u00e9m, foi habilitado o servi\u00e7o de DHCP diretamente no MAAS Controller, com os seguintes ajustes:</p> <ul> <li> <p>Faixa reservada: de 172.16.11.1 at\u00e9 172.16.14.255</p> </li> <li> <p>DNS da subrede: 172.20.129.131</p> </li> </ul> <p>Obs:</p> <p>A integridade do ambiente foi confirmada por meio do painel de controladores do MAAS, onde todos os servi\u00e7os essenciais, incluindo regiond, rackd e dhcpd, apresentaram status verde.</p>"},{"location":"roteiro1/main/#25-comissionamento-das-maquinas","title":"2.5 Comissionamento das m\u00e1quinas","text":"<p>As NUCs server1 a server5 foram registradas como hosts no MAAS. Todas as m\u00e1quinas realizaram o boot via PXE e foram comissionadas com sucesso. Ap\u00f3s o processo, passaram a apresentar status Ready, com todas as informa\u00e7\u00f5es de hardware corretamente detectadas.</p>"},{"location":"roteiro1/main/#26-criacao-da-ovs-bridge-br-ex-e-configuracao-do-nat","title":"2.6 Cria\u00e7\u00e3o da OVS bridge (br-ex) e configura\u00e7\u00e3o do NAT","text":"<p>Foi criada uma Open vSwitch bridge (OVS) chamada br-ex em cada n\u00f3 de nuvem, utilizando a interface f\u00edsica enp1s0. Essa ponte foi configurada no painel do MAAS, na aba Network de cada m\u00e1quina. A configura\u00e7\u00e3o de br-ex \u00e9 essencial para o funcionamento futuro do OVN e para suportar redes overlay sem exigir m\u00faltiplas interfaces f\u00edsicas por n\u00f3.</p> <p>Finalmente, configuramos um NAT no roteador do kit, liberando acesso externo \u00e0 m\u00e1quina principal (main) pela porta 22. Isso permitiu o uso de SSH mesmo fora da rede local.</p> <p>Tamb\u00e9m foi criada uma regra de gest\u00e3o para o endere\u00e7o 0.0.0.0/0, permitindo acesso remoto \u00e0 interface de gerenciamento do pr\u00f3prio roteador.</p>"},{"location":"roteiro1/main/#3-aplicacao","title":"3. Aplica\u00e7\u00e3o","text":"<p>Fizemos a realiza\u00e7\u00e3o de um deploy manual para uma aplica\u00e7\u00e3o simples na nuvem MaaS da nossa dupla.</p>"},{"location":"roteiro1/main/#31-ajuste-de-dns-para-deploy-bare-metal","title":"3.1 Ajuste de DNS para deploy Bare Metal","text":"<p>Ajustamos o DNS da nossa rede bare-metal diretamente pelo MAAS. Na aba Subnets, acessamos a subnet 172.16.0.0/20, editamos o campo Subnet summary e substitu\u00edmos o DNS para o do Insper: 172.20.129.131.</p>"},{"location":"roteiro1/main/#32-deploy-manual-do-banco-de-dados-postgresql-no-server1","title":"3.2 Deploy manual do banco de dados PostgreSQL no server1","text":"<p>A partir do dashboard do MAAS, realizamos o deploy do Ubuntu 22.04 no server1. Ap\u00f3s o deploy, acessamos o terminal via SSH:</p> <pre><code>$ ssh cloud@172.16.15.1\n</code></pre> <p>No terminal, executamos a instala\u00e7\u00e3o do PostgreSQL:</p> <pre><code>$ sudo apt update\n$ sudo apt install postgresql postgresql-contrib -y\n</code></pre> <p>Em seguida, configuramos o banco:</p> <pre><code>$ sudo su - postgres\n$ createuser -s cloud -W          \n$ createdb -O cloud tasks\n</code></pre> <p>Alteramos os arquivos de configura\u00e7\u00e3o para permitir conex\u00f5es externas:</p> <pre><code>$ nano /etc/postgresql/14/main/postgresql.conf\n# linha: listen_addresses = '*'\n\n$ nano /etc/postgresql/14/main/pg_hba.conf\n# linha adicionada: host all all 172.16.0.0/20 trust\n</code></pre> <p>Por fim, reiniciamos o servi\u00e7o e liberamos a porta padr\u00e3o do PostgreSQL:</p> <pre><code>$ sudo ufw allow 5432/tcp\n$ sudo systemctl restart postgresql\n</code></pre>"},{"location":"roteiro1/main/#tarefa-1-validacao-do-banco-de-dados-postgresql","title":"Tarefa 1 - Valida\u00e7\u00e3o do banco de dados PostgreSQL","text":"<ol> <li>Verifica\u00e7\u00e3o do status do PostgreSQL no server1</li> </ol> <p>O comando sudo systemctl status postgresql foi executado no terminal da m\u00e1quina server1. A sa\u00edda mostra que o servi\u00e7o est\u00e1 com o status active (exited), indicando que o PostgreSQL est\u00e1 corretamente habilitado e funcionando no sistema operacional. Isso comprova que o banco de dados foi inicializado com sucesso e est\u00e1 pronto para receber conex\u00f5es.</p> <p></p> <ol> <li>Teste de conex\u00e3o local com o banco de dados PostgreSQL</li> </ol> <p>Na imagem, foi utilizado o comando psql -U cloud -h 172.16.15.0 tasks dentro do pr\u00f3prio server1 para acessar o banco de dados tasks. A autentica\u00e7\u00e3o foi feita com sucesso utilizando o usu\u00e1rio cloud, e a conex\u00e3o foi estabelecida diretamente com o IP da m\u00e1quina local. A presen\u00e7a do prompt tasks=# confirma que a conex\u00e3o est\u00e1 ativa e funcional.</p> <p></p> <ol> <li>Acess\u00edvel a partir da m\u00e1quina MAIN</li> </ol> <p>Mostra o processo de instala\u00e7\u00e3o do postgresql-client, que \u00e9 o cliente necess\u00e1rio para fazer conex\u00e3o externa ao banco de dados PostgreSQL, partindo da m\u00e1quina main.</p> <p></p>"},{"location":"roteiro1/main/#33-deploy-da-aplicacao-django-no-server2-via-maas-cli","title":"3.3 Deploy da aplica\u00e7\u00e3o Django no server2 via MAAS CLI","text":"<p>Acessamos o terminal do main e realizamos login no MAAS:</p> <pre><code>$ maas login cloud http://172.16.0.3:5240/MAAS/\n</code></pre> <p>Solicitamos a aloca\u00e7\u00e3o do server2:</p> <pre><code>$ maas cloud machines allocate name=server2\n</code></pre> <p>Com o system_id retornado, iniciamos o deploy:</p> <pre><code>$ maas cloud machine deploy [system_id]\n</code></pre> <p>Acessamos o server2 via SSH e iniciamos o processo de instala\u00e7\u00e3o da aplica\u00e7\u00e3o:</p> <pre><code>$ git clone https://github.com/raulikeda/tasks.git\n$ cd tasks\n$ ./install.sh\n$ sudo reboot\n</code></pre>"},{"location":"roteiro1/main/#34-acesso-externo-via-tunel-ssh","title":"3.4 Acesso externo via t\u00fanel SSH","text":"<p>Com o servi\u00e7o Django rodando na porta 8080 do server2, realizamos o acesso externo usando um t\u00fanel SSH a partir do main:</p> <pre><code>$ ssh cloud@10.103.0.X -L 8001:172.16.15.2:8080\n</code></pre> <p>Abrimos o navegador e acessamos:</p> <pre><code>http://localhost:8001/admin/\n</code></pre>"},{"location":"roteiro1/main/#tarefa-2","title":"Tarefa 2","text":"<ol> <li>Comprova\u00e7\u00e3o da Infraestrutura MaaS e Imagens</li> </ol> <p>A imagem exibe cinco m\u00e1quinas registradas no MAAS. Dentre elas, duas (server1 e server2) est\u00e3o no estado Deployed, com IPs vis\u00edveis (172.16.15.0 e 172.16.15.6), o que confirma que j\u00e1 passaram pelo processo de deploy com a imagem do Ubuntu 22.04 LTS. As demais (server3, server4 e server5) est\u00e3o em estado Ready, prontas para serem utilizadas.</p> <p></p> <ol> <li>Sincroniza\u00e7\u00e3o da imagem Ubuntu 22.04 LTS no MAAS</li> </ol> <p>A imagem da vers\u00e3o 22.04 LTS do Ubuntu (arquitetura amd64) foi devidamente baixada e sincronizada com sucesso, estando marcada como Synced. Isso confirma que a infraestrutura est\u00e1 pronta para realizar deploys com essa vers\u00e3o nas m\u00e1quinas dispon\u00edveis no pool do kit.</p> <p></p> <ol> <li>Comissionamento das m\u00e1quina com status \u201cPassed\u201d</li> </ol> <p>Agora, vemos os testes de comissionamento realizados nas 5 m\u00e1quinasTodos os testes \u2014 incluindo verifica\u00e7\u00e3o de interfaces de rede, informa\u00e7\u00f5es de hardware, portas seriais e hints de configura\u00e7\u00e3o \u2014 retornaram com status \u201cPassed\u201d, o que garante que o n\u00f3 foi corretamente detectado e est\u00e1 apto a ser utilizado na nuvem bare-metal. </p> <p>Server 1:</p> <p></p> <p>Server 2:</p> <p></p> <p>Server 3: </p> <p></p> <p>Server 4: </p> <p></p> <p>Server 5:</p> <p></p>"},{"location":"roteiro1/main/#35-deploy-automatizado-da-aplicacao-no-server3-com-ansible","title":"3.5 Deploy automatizado da aplica\u00e7\u00e3o no server3 com Ansible","text":"<p>Realizamos a instala\u00e7\u00e3o do Ansible no main:</p> <pre><code>$ sudo apt install ansible\n$ wget https://raw.githubusercontent.com/raulikeda/tasks/master/tasks-install-playbook.yaml\n</code></pre> <p>Solicitamos o deploy do server3 via MAAS CLI e rodamos o playbook:</p> <pre><code>$ maas cloud machines allocate name=server3\n$ maas cloud machine deploy [system_id]\n\n$ ansible-playbook tasks-install-playbook.yaml --extra-vars server=172.16.15.3\n</code></pre> <p>A aplica\u00e7\u00e3o foi implantada automaticamente, conectando-se ao banco do server1.</p>"},{"location":"roteiro1/main/#tarefa-3","title":"Tarefa 3","text":"<ol> <li>M\u00e1quinas alocadas no MAAS ap\u00f3s deploy da aplica\u00e7\u00e3o</li> </ol> <p>Vemos que tr\u00eas m\u00e1quinas (server1, server2 e server3) est\u00e3o com status Deployed, indicando que foram utilizadas para banco de dados e aplica\u00e7\u00f5es Django.</p> <p></p> <ol> <li>Aplica\u00e7\u00e3o Django em execu\u00e7\u00e3o no server2</li> </ol> <p>O print mostra o Hello, world. You're at the tasks index. retornado pela aplica\u00e7\u00e3o Django, acessada via navegador local utilizando localhost:8001/tasks. Isso confirma que a aplica\u00e7\u00e3o est\u00e1 funcional e acess\u00edvel.</p> <p></p> <ol> <li>Explica\u00e7\u00e3o do Deploy manual</li> </ol> <p>Para realizar o deploy manual da aplica\u00e7\u00e3o Django no server2, primeiramente solicitamos a aloca\u00e7\u00e3o da m\u00e1quina via MAAS CLI. Em seguida, executamos os comandos para clonar o reposit\u00f3rio da aplica\u00e7\u00e3o e iniciar o processo de instala\u00e7\u00e3o via o script install.sh. O servi\u00e7o foi configurado para escutar na porta 8080, e o acesso externo foi poss\u00edvel atrav\u00e9s da cria\u00e7\u00e3o de um t\u00fanel SSH a partir do main, redirecionando o tr\u00e1fego da porta 8001 local para a porta 8080 do server2. Esse procedimento permitiu validar o funcionamento completo da aplica\u00e7\u00e3o hospedada na m\u00e1quina provisionada.</p>"},{"location":"roteiro1/main/#36-configuracao-do-load-balancer-com-nginx-no-server4","title":"3.6 Configura\u00e7\u00e3o do Load Balancer com NGINX no server4","text":"<p>Instalamos o NGINX no server4:</p> <pre><code>$ sudo apt-get install nginx\n</code></pre> <p>Editamos o arquivo de configura\u00e7\u00e3o padr\u00e3o:</p> <pre><code>$ sudo nano /etc/nginx/sites-available/default\n</code></pre> <p>Adicionamos o m\u00f3dulo de balanceamento:</p> <p><pre><code>upstream backend {\n    server 172.16.15.2:8080;\n    server 172.16.15.3:8080;\n}\n\nserver {\n    listen 80;\n    location / {\n        proxy_pass http://backend;\n    }\n}\n</code></pre> Reiniciamos o servi\u00e7o:</p> <pre><code>$ sudo service nginx restart\n</code></pre> <p>Modificamos o conte\u00fado da fun\u00e7\u00e3o index() no arquivo tasks/views.py de cada servidor para exibir uma mensagem personalizada, identificando qual servidor respondeu a requisi\u00e7\u00e3o.</p>"},{"location":"roteiro1/main/#tarefa-4","title":"Tarefa 4","text":"<ol> <li>Acesso ao Server2 via t\u00fanel SSH</li> </ol> <p>A imagem mostra o terminal local conectado ao server2 via SSH. Este t\u00fanel permite o redirecionamento do tr\u00e1fego da porta 8001 do computador local para a porta 8080 do server2, onde a aplica\u00e7\u00e3o Django est\u00e1 hospedada. Vendo o texto padr\u00e3o no fundo, confirmamos o funcionamento da aplica\u00e7\u00e3o Django no Server 2.</p> <p></p> <ol> <li>Acesso ao Server3 via t\u00fanel SSH</li> </ol> <p>O mesmo racional do acesso ao server 2, mas para o server3.</p> <p></p> <ol> <li>Diferen\u00e7a entre instala\u00e7\u00e3o manual e via Ansible</li> </ol> <p>A instala\u00e7\u00e3o manual exige executar cada passo diretamente no terminal (como clonar o reposit\u00f3rio, rodar install.sh, configurar depend\u00eancias e reiniciar). J\u00e1 com o Ansible, tudo \u00e9 feito automaticamente por meio de um playbook, o que economiza tempo, reduz erros e garante que o processo seja id\u00eantico em v\u00e1rios servidores.</p>"},{"location":"roteiro1/main/#tarefa-5","title":"Tarefa 5","text":"<ol> <li>Quatro m\u00e1quinas no estado Deployed com IPs vis\u00edveis no MAAS</li> </ol> <p>Vemos o dashboard do MAAS com quatro m\u00e1quinas no estado Deployed (server1, server2, server3 e server4), cada uma com seu respectivo IP vis\u00edvel. Essa visualiza\u00e7\u00e3o confirma que os n\u00f3s est\u00e3o devidamente provisionados com Ubuntu 22.04 LTS e prontos para opera\u00e7\u00e3o, incluindo o server4, que atuar\u00e1 como balanceador de carga via NGINX.</p> <p></p> <ol> <li>GET request mostra resposta da aplica\u00e7\u00e3o no server3 e server 2 via proxy reverso</li> </ol> <p>Por fim, notamos o conte\u00fado acessado via localhost:8001/tasks/, que est\u00e1 sendo redirecionado atrav\u00e9s do proxy reverso configurado no server4 (NGINX). As mensagens \"Ol\u00e1, voc\u00ea est\u00e1 no server3.\" e \"Ol\u00e1, voc\u00ea est\u00e1 no server2.\" confirmam funcionamento do balanceamento de carga via NGINX. </p> <p>Server3: </p> <p></p> <p>Server2:</p> <p></p>"},{"location":"roteiro1/main/#37-finalizacao-e-limpeza-do-ambiente","title":"3.7 Finaliza\u00e7\u00e3o e limpeza do ambiente","text":"<p>Ap\u00f3s os testes, realizamos o release de todos os servidores do kit atrav\u00e9s do dashboard do MAAS. Esse processo liberou os recursos de hardware utilizados para as aplica\u00e7\u00f5es e preparou o ambiente para futuros testes.</p>"},{"location":"roteiro2/main/","title":"Roteiro 2 - Juju","text":""},{"location":"roteiro2/main/#1-objetivo","title":"1. Objetivo","text":"<p>Tendo compreendido o relat\u00f3rio 1, o presente relat\u00f3rio tem como alvo os seguintes objetivos com Deployment Orchestration:</p> <p>\u2022   Automatizar a infraestrutura, reduzindo a complexidade e garantindo consist\u00eancia nas implanta\u00e7\u00f5es.</p> <p>\u2022   Gerenciamento centralizado, permitindo administra\u00e7\u00e3o eficiente de m\u00faltiplos servidores.</p> <p>\u2022   Integra\u00e7\u00e3o com provedores, usando recursos f\u00edsicos de forma otimizada.</p>"},{"location":"roteiro2/main/#2-infraestrutura","title":"2. Infraestrutura","text":""},{"location":"roteiro2/main/#21-por-que-utilizar-o-juju-para-deployment-orchestration","title":"2.1 Por que utilizar o Juju para Deployment Orchestration?","text":"<p>O Juju \u00e9 uma ferramenta de orquestra\u00e7\u00e3o que atua desde o provisionamento at\u00e9 a configura\u00e7\u00e3o e integra\u00e7\u00e3o de servi\u00e7os. Ao contr\u00e1rio do Ansible, que se limita \u00e0 configura\u00e7\u00e3o de m\u00e1quinas j\u00e1 provisionadas, o Juju se integra diretamente com o MAAS (Metal as a Service), permitindo orquestrar o ambiente Bare Metal de forma completa e automatizada.</p>"},{"location":"roteiro2/main/#22-instalacao-do-juju","title":"2.2 Instala\u00e7\u00e3o do Juju","text":"<p>Inicialmente, foi necess\u00e1rio acessar o dashboard do MAAS e garantir que todas as m\u00e1quinas nomeadas como server1, server2, server3, server4 e server5 estivessem com status Ready. Realizamos o release das m\u00e1quinas que haviam sido anteriormente utilizadas para as aplica\u00e7\u00f5es Django e PostgreSQL, a fim de garantir que estivessem dispon\u00edveis para novas aloca\u00e7\u00f5es.</p> <p>Tendo as m\u00e1quinas prontas,  acessamos o n\u00f3 principal (main) via SSH para a instala\u00e7\u00e3o do Juju. A instala\u00e7\u00e3o foi realizada utilizando o Snap com o canal da vers\u00e3o 3.6:</p> <pre><code>$ sudo snap install juju --channel 3.6\n</code></pre>"},{"location":"roteiro2/main/#23-configuracao-da-cloud-maas-no-juju","title":"2.3 Configura\u00e7\u00e3o da Cloud MAAS no Juju","text":"<p>Ap\u00f3s a instala\u00e7\u00e3o do Juju, foi necess\u00e1rio garantir que o MAAS estivesse vis\u00edvel como provedor de cloud. Para isso, executamos o comando:</p> <pre><code>$ juju clouds\n</code></pre> <p>\u00c9 importante notar que:</p> <p>Como o MAAS n\u00e3o apareceu na listagem, adicionamos manualmente uma nova cloud por meio de um arquivo de configura\u00e7\u00e3o chamado maas-cloud.yaml, com o seguinte conte\u00fado:</p> <pre><code>clouds:\n  maas-one:\n    type: maas\n    auth-types: [oauth1]\n    endpoint: http://192.168.0.3:5240/MAAS/\n</code></pre> <p>Em seguida, adicionamos a cloud com o comando:</p> <pre><code>$ juju add-cloud --client -f maas-cloud.yaml maas-one\n</code></pre>"},{"location":"roteiro2/main/#24-adicao-de-credenciais-ao-juju","title":"2.4 Adi\u00e7\u00e3o de credenciais ao Juju","text":"<p>Para permitir que o Juju se autenticasse e interagisse com o MAAS, criamos o arquivo maas-creds.yaml, contendo as credenciais de autentica\u00e7\u00e3o:</p> <pre><code>credentials:\n  maas-one:\n    anyuser:\n      auth-type: oauth1\n      maas-oauth: &lt;API KEY&gt;\n</code></pre> <p>O valor  foi substitu\u00eddo pela chave gerada no MAAS, dispon\u00edvel no menu do usu\u00e1rio. <p>As credenciais foram adicionadas com o comando:</p> <pre><code>$ juju add-credential --client -f maas-creds.yaml maas-one\n</code></pre>"},{"location":"roteiro2/main/#25-bootstrap-do-controller-do-juju","title":"2.5 Bootstrap do controller do Juju","text":"<p>Para iniciar o uso do Juju, foi necess\u00e1rio criar um controller, respons\u00e1vel por gerenciar os deploys futuros. Antes do bootstrap, acessamos o dashboard do MAAS e aplicamos a tag juju \u00e0 m\u00e1quina server1.</p> <p>O comando utilizado para o bootstrap foi:</p> <pre><code>$ juju bootstrap --bootstrap-series=jammy --constraints tags=juju maas-one maas-controller\n</code></pre> <p>Esse processo levou alguns minutos, pois envolveu o provisionamento da m\u00e1quina e a instala\u00e7\u00e3o do agente de controle do Juju. Ao final, validamos o sucesso da opera\u00e7\u00e3o com:</p> <pre><code>$ juju status\n</code></pre> <p>O controller foi iniciado com sucesso e passou a ser a interface principal entre os comandos do juju-cli e a infraestrutura gerenciada via MAAS.</p>"},{"location":"roteiro2/main/#3-aplicacao","title":"3. Aplica\u00e7\u00e3o","text":""},{"location":"roteiro2/main/#31-configuracao-do-modelo-para-deploy-das-aplicacoes","title":"3.1 Configura\u00e7\u00e3o do modelo para deploy das aplica\u00e7\u00f5es","text":"<p>Com o controller j\u00e1 inicializado, realizamos a instala\u00e7\u00e3o do Dashboard do Juju - o que permitiu a visualiza\u00e7\u00e3o gr\u00e1fica dos modelos, aplica\u00e7\u00f5es e unidades gerenciadas;</p> <p>Ap\u00f3s esse acesso, retornamos ao terminal e listamos os modelos dispon\u00edveis com:</p> <pre><code>$ juju models\n</code></pre> <p>Em seguida, foi feito o switch para o modelo padr\u00e3o do controller com o comando:</p> <pre><code>$ juju switch maas-controller:admin/maas\n</code></pre> <p>Dessa forma, garantimos que Grafana e Prometheus fossem implantados no modelo correto.</p>"},{"location":"roteiro2/main/#32-preparacao-dos-charms-locais","title":"3.2 Prepara\u00e7\u00e3o dos charms locais","text":"<p>Para realizar o deploy local das aplica\u00e7\u00f5es, foi criada uma pasta dedicada para armazenar os charms baixados do Charmhub:</p> <p><pre><code>$ mkdir -p /home/cloud/charms\n$ cd /home/cloud/charms\n</code></pre> Em seguida, baixamos:</p> <pre><code>$ juju download grafana\n$ juju download prometheus2\n</code></pre>"},{"location":"roteiro2/main/#33-deploy-dos-servicos","title":"3.3 Deploy dos servi\u00e7os","text":"<p>Com a utiliza\u00e7\u00e3o do charm local, fizemos:</p> <p><pre><code>$ juju deploy ./prometheus2_XXX.charm\n$ juju deploy ./grafana_XXX.charm\n</code></pre> Note que:</p> <p>Enquanto ocorria o deploy, observamos analisando o status do Juju.</p>"},{"location":"roteiro2/main/#34-integracao-validacao-e-visualizacao-dos-servicos","title":"3.4 Integra\u00e7\u00e3o, valida\u00e7\u00e3o e visualiza\u00e7\u00e3o dos servi\u00e7os","text":"<p>Com os servi\u00e7os Grafana e Prometheus em estado active, realizamos a integra\u00e7\u00e3o entre eles utilizando os procedimentos descritos no README do charm do Grafana. A integra\u00e7\u00e3o foi feita por meio da interface gr\u00e1fica do Grafana, acessando o painel, criando um novo dashboard e configurando o Prometheus como fonte de dados (source).</p> <p>Na sequ\u00eancia, validamos o funcionamento da aplica\u00e7\u00e3o acessando o dashboard do Grafana a partir da rede do Insper. A conex\u00e3o foi estabelecida com sucesso, comprovando a disponibilidade da aplica\u00e7\u00e3o fora do ambiente local.</p> <p>Por fim, acessamos a interface gr\u00e1fica do Juju, visualisando as aplica\u00e7\u00f5es implantadas e em execu\u00e7\u00e3o dentro do modelo ativo.</p>"},{"location":"roteiro2/main/#tarefa","title":"Tarefa","text":""},{"location":"roteiro2/main/#1-visualizacao-das-maquinas-no-dashboard-do-maas","title":"1. Visualiza\u00e7\u00e3o das m\u00e1quinas no Dashboard do MAAS","text":"<p>A imagem abaixo mostra o painel do MAAS com as cinco m\u00e1quinas cadastradas, onde duas est\u00e3o com status Ready (prontas para uso) e tr\u00eas est\u00e3o em estado Deployed, indicando que est\u00e3o em opera\u00e7\u00e3o. \u00c9 poss\u00edvel visualizar tamb\u00e9m os IPs atribu\u00eddos a cada m\u00e1quina ativa.</p> <p></p>"},{"location":"roteiro2/main/#2-verificacao-do-estado-dos-servicos-com-juju-status","title":"2. Verifica\u00e7\u00e3o do estado dos servi\u00e7os com juju status","text":"<p>O comando juju status foi executado ap\u00f3s o deploy completo das aplica\u00e7\u00f5es. A imagem a seguir mostra que tanto o Grafana quanto o Prometheus est\u00e3o no estado active, indicando que os servi\u00e7os est\u00e3o operando corretamente. Tamb\u00e9m \u00e9 poss\u00edvel verificar as portas abertas e os IPs atribu\u00eddos.</p> <p></p>"},{"location":"roteiro2/main/#3-integracao-entre-grafana-e-prometheus-no-dashboard","title":"3. Integra\u00e7\u00e3o entre Grafana e Prometheus no dashboard","text":"<p>Nesta etapa, foi acessado o dashboard do Grafana, onde foi criado um painel que utiliza o Prometheus como fonte de dados (source). A imagem mostra o gr\u00e1fico gerado a partir da consulta realizada no Prometheus.</p> <p></p>"},{"location":"roteiro2/main/#4-acesso-ao-grafana-a-partir-da-rede-do-insper","title":"4. Acesso ao Grafana a partir da rede do Insper","text":"<p>A pr\u00f3xima imagem comprova o acesso ao dashboard do Grafana a partir da rede do Insper, como evidenciado pela conex\u00e3o ativa \u00e0 rede \"Insper_Alunos\". Isso demonstra que a aplica\u00e7\u00e3o est\u00e1 acess\u00edvel fora da rede local do KIT.</p> <p></p>"},{"location":"roteiro2/main/#5-visualizacao-das-aplicacoes-em-execucao-no-dashboard-do-juju","title":"5. Visualiza\u00e7\u00e3o das aplica\u00e7\u00f5es em execu\u00e7\u00e3o no Dashboard do Juju","text":"<p>Por fim, acessamos a interface gr\u00e1fica do Juju para visualizar o modelo em uso e confirmar que as aplica\u00e7\u00f5es est\u00e3o sendo gerenciadas corretamente. A imagem mostra o Grafana e o Prometheus com status Running, ambos implantados localmente.</p> <p></p>"},{"location":"roteiro3/main/","title":"Roteiro 3 \u2013 Private Cloud com OpenStack","text":""},{"location":"roteiro3/main/#1-objetivos","title":"1. Objetivos","text":"<p>Este roteiro d\u00e1 sequ\u00eancia aos anteriores (MAAS e Juju) e foca na cria\u00e7\u00e3o de uma nuvem privada baseada em OpenStack, usando o mesmo kit de servidores. Aqui definimos o que ser\u00e1 feito e por qu\u00ea. Assim \u00e9 necess\u00e1rio:</p> <ul> <li> <p>Reconhecer o que \u00e9 uma nuvem privada e por que us\u00e1-la dentro do laborat\u00f3rio.</p> </li> <li> <p>Entender, de forma b\u00e1sica, os principais blocos do OpenStack (controle, computa\u00e7\u00e3o, rede e armazenamento).</p> </li> <li> <p>Montar a infraestrutura f\u00edsica m\u00ednima necess\u00e1ria para o cluster.</p> </li> <li> <p>Instalar o OpenStack (via Juju + MAAS) em modo funcional, ainda que n\u00e3o-HA.</p> </li> <li> <p>Testar a cria\u00e7\u00e3o de uma VM, rede interna e volume para validar o ambiente.</p> </li> </ul>"},{"location":"roteiro3/main/#2-infraestrutura","title":"2. Infraestrutura","text":"<p>Neste roteiro, instalamos o OpenStack sobre o kit, continuando a usar o MAAS para orquestrar o bare-metal e o Juju para implantar os servi\u00e7os; isso permitiu criar e distribuir VMs de modo eficiente entre todos os n\u00f3s. A implanta\u00e7\u00e3o seguiu o guia oficial do OpenStack, j\u00e1 adaptado ao nosso ambiente:</p> <ul> <li> <p>Pap\u00e9is dos n\u00f3s \u2013 server1 atuou como controller (e hospedou o Juju Controller); server2 ficou reservado para conting\u00eancia; server3-5 funcionaram como compute.</p> </li> <li> <p>Monitoramento em tempo real \u2013 em um terminal extra, executamos:</p> </li> </ul> <p><pre><code>watch -n 2 --color \"juju status --color\"\n</code></pre> para acompanhar a instala\u00e7\u00e3o a cada 2 s.</p> <p>Com essa estrutura definida, prosseguimos para os detalhes de hardware, rede e mapeamento de servi\u00e7os.</p>"},{"location":"roteiro3/main/#21-juju-controller","title":"2.1 Juju Controller","text":"<p>Instalamos o Juju Controller no server1, ap\u00f3s aplicar a tag controller, usando:</p> <p><pre><code>juju bootstrap --bootstrap-series=jammy \\\n  --constraints tags=controller \\\n  maas-one maas-controller\n</code></pre> Esse n\u00f3 passou a gerenciar todo o deploy do OpenStack.</p>"},{"location":"roteiro3/main/#22-modelo-de-deploy","title":"2.2 Modelo de deploy","text":"<p>Criamos o modelo openstack e mudamos para ele:</p> <p><pre><code>juju add-model --config default-series=jammy openstack\njuju switch maas-controller:openstack\n</code></pre> (O Juju Dashboard p\u00f4de ser habilitado opcionalmente.)</p>"},{"location":"roteiro3/main/#23-ceph-osd","title":"2.3 Ceph OSD","text":"<p>Preparamos o arquivo ceph-osd.yaml indicando os discos (/dev/sda /dev/sdb) e implantamos tr\u00eas unidades \u2014 uma em cada n\u00f3 compute (server3-5):</p> <pre><code>juju deploy -n 3 \\\n  --channel quincy/stable \\\n  --config ceph-osd.yaml \\\n  --constraints tags=compute \\\n  ceph-osd\n</code></pre>"},{"location":"roteiro3/main/#24-nova-compute","title":"2.4 Nova Compute","text":"<p>Criamos nova-compute.yaml (live-migration, resize, QEMU, etc.) e distribu\u00edmos tr\u00eas unidades de nova-compute nos mesmos n\u00f3s compute:</p> <pre><code>juju deploy -n 3 \\\n  --to 0,1,2 \\\n  --channel yoga/stable \\\n  --config nova-compute.yaml \\\n  nova-compute\n</code></pre>"},{"location":"roteiro3/main/#25-mysql-innodb-cluster","title":"2.5 MySQL InnoDB Cluster","text":"<p>Implantamos o banco de dados MySQL em cont\u00eaineres LXD sobre os tr\u00eas n\u00f3s compute:</p> <pre><code>juju deploy -n 3 \\\n  --to lxd:0,lxd:1,lxd:2 \\\n  --channel 8.0/stable \\\n  mysql-innodb-cluster\n</code></pre>"},{"location":"roteiro3/main/#26-vault","title":"2.6 Vault","text":"<p>Implantamos Vault num cont\u00eainer LXD do server5, configurando o roteamento MySQL e integrando-o ao cluster:</p> <pre><code>juju deploy --to lxd:2 --channel 1.8/stable vault\njuju deploy --channel 8.0/stable mysql-router vault-mysql-router\njuju integrate vault-mysql-router:db-router      mysql-innodb-cluster:db-router\njuju integrate vault-mysql-router:shared-db      vault:shared-db\njuju run vault/leader generate-root-ca\njuju integrate mysql-innodb-cluster:certificates vault:certificates\n</code></pre> <p>Ap\u00f3s isso, inicializamos e unsealamos o Vault conforme a documenta\u00e7\u00e3o.</p>"},{"location":"roteiro3/main/#27-neutron-ovn","title":"2.7 Neutron + OVN","text":"<p>Criamos neutron.yaml com mapeamentos de bridge (br-ex) e provider physnet1. Implantamos:</p> <pre><code>juju deploy -n 3 --to lxd:0,lxd:1,lxd:2 \\\n  --channel 22.03/stable ovn-central\n\njuju deploy --to lxd:1 --channel yoga/stable \\\n  --config neutron.yaml neutron-api\n\njuju deploy --channel yoga/stable   neutron-api-plugin-ovn\njuju deploy --channel 22.03/stable  --config neutron.yaml ovn-chassis\n</code></pre> <p>Integra\u00e7\u00f5es principais:</p> <pre><code>juju integrate neutron-api-plugin-ovn:neutron-plugin neutron-api:neutron-plugin-api-subordinate\njuju integrate neutron-api-plugin-ovn:ovsdb-cms ovn-central:ovsdb-cms\njuju integrate ovn-chassis:ovsdb ovn-central:ovsdb\njuju integrate ovn-chassis:nova-compute nova-compute:neutron-plugin\nfor app in neutron-api neutron-api-plugin-ovn ovn-central ovn-chassis; do\n  juju integrate $app:certificates vault:certificates\ndone\njuju deploy --channel 8.0/stable mysql-router neutron-api-mysql-router\njuju integrate neutron-api-mysql-router:db-router  mysql-innodb-cluster:db-router\njuju integrate neutron-api-mysql-router:shared-db neutron-api:shared-db\n</code></pre>"},{"location":"roteiro3/main/#28-keystone","title":"2.8 Keystone","text":"<p>Implantamos Keystone em cont\u00eainer LXD do server3 e o conectamos ao banco e aos servi\u00e7os:</p> <pre><code>juju deploy --to lxd:0 --channel yoga/stable keystone\njuju deploy --channel 8.0/stable mysql-router keystone-mysql-router\njuju integrate keystone-mysql-router:db-router mysql-innodb-cluster:db-router\njuju integrate keystone-mysql-router:shared-db keystone:shared-db\njuju integrate keystone:identity-service neutron-api:identity-service\njuju integrate keystone:certificates       vault:certificates\n</code></pre>"},{"location":"roteiro3/main/#29-rabbitmq","title":"2.9 RabbitMQ","text":"<p>Por fim, provisionamos RabbitMQ em cont\u00eainer LXD do server5 e ligamos suas filas aos servi\u00e7os de rede e compute:</p> <pre><code>juju deploy --to lxd:2 --channel 3.9/stable rabbitmq-server\njuju integrate rabbitmq-server:amqp neutron-api:amqp\njuju integrate rabbitmq-server:amqp nova-compute:amqp\n</code></pre> <p>Com todos esses passos conclu\u00eddos e monitorados por meio de:</p> <pre><code>watch -n 2 --color \"juju status --color\"\n</code></pre> <p>os units chegaram a active/idle, validando a instala\u00e7\u00e3o b\u00e1sica do OpenStack.</p>"},{"location":"roteiro3/main/#210-nova-cloud-controller","title":"2.10 Nova Cloud Controller","text":"<p>Criamos o arquivo ncc.yaml (network-manager: Neutron) e implantamos o servi\u00e7o Nova Cloud Controller em um cont\u00eainer LXD do server5:</p> <pre><code>juju deploy --to lxd:2 --channel yoga/stable --config ncc.yaml nova-cloud-controller\n\njuju deploy --channel 8.0/stable mysql-router ncc-mysql-router\njuju integrate ncc-mysql-router:db-router   mysql-innodb-cluster:db-router\njuju integrate ncc-mysql-router:shared-db   nova-cloud-controller:shared-db\n\njuju integrate nova-cloud-controller:identity-service keystone:identity-service\njuju integrate nova-cloud-controller:amqp            rabbitmq-server:amqp\njuju integrate nova-cloud-controller:neutron-api      neutron-api:neutron-api\njuju integrate nova-cloud-controller:cloud-compute    nova-compute:cloud-compute\njuju integrate nova-cloud-controller:certificates     vault:certificates\n</code></pre>"},{"location":"roteiro3/main/#211-placement","title":"2.11 Placement","text":"<p>Implantamos o servi\u00e7o Placement, tamb\u00e9m em cont\u00eainer LXD do server5, e o integramos ao banco de dados:</p> <pre><code>juju deploy --to lxd:2 --channel yoga/stable placement\n\njuju deploy --channel 8.0/stable mysql-router placement-mysql-router\njuju integrate placement-mysql-router:db-router  mysql-innodb-cluster:db-router\njuju integrate placement-mysql-router:shared-db placement:shared-db\n\njuju integrate placement:identity-service keystone:identity-service\njuju integrate placement:placement         nova-cloud-controller:placement\njuju integrate placement:certificates      vault:certificates\n</code></pre>"},{"location":"roteiro3/main/#212-horizon-dashboard","title":"2.12 Horizon (Dashboard)","text":"<p>Implantamos o Horizon em outro cont\u00eainer LXD do server5:</p> <pre><code>juju deploy --to lxd:2 --channel yoga/stable openstack-dashboard\n\njuju deploy --channel 8.0/stable mysql-router dashboard-mysql-router\njuju integrate dashboard-mysql-router:db-router  mysql-innodb-cluster:db-router\njuju integrate dashboard-mysql-router:shared-db openstack-dashboard:shared-db\n\njuju integrate openstack-dashboard:identity-service keystone:identity-service\njuju integrate openstack-dashboard:certificates   vault:certificates\n</code></pre>"},{"location":"roteiro3/main/#213-glance","title":"2.13 Glance","text":"<p>Implantamos o Glance no mesmo host LXD (server5) e o configuramos para usar Ceph:</p> <pre><code>juju deploy --to lxd:2 --channel yoga/stable glance\n\njuju deploy --channel 8.0/stable mysql-router glance-mysql-router\njuju integrate glance-mysql-router:db-router  mysql-innodb-cluster:db-router\njuju integrate glance-mysql-router:shared-db glance:shared-db\n\njuju integrate glance:image-service    nova-cloud-controller:image-service\njuju integrate glance:image-service    nova-compute:image-service\njuju integrate glance:identity-service keystone:identity-service\njuju integrate glance:certificates     vault:certificates\n</code></pre>"},{"location":"roteiro3/main/#214-ceph-monitor","title":"2.14 Ceph Monitor","text":"<p>Criamos ceph-mon.yaml com expected-osd-count: 3 e implantamos tr\u00eas monitores Ceph em cont\u00eaineres LXD (um em cada n\u00f3 compute):</p> <pre><code>juju deploy -n 3 --to lxd:0,lxd:1,lxd:2 \\\n     --channel quincy/stable --config ceph-mon.yaml ceph-mon\n\njuju integrate ceph-mon:osd    ceph-osd:mon\njuju integrate ceph-mon:client nova-compute:ceph\njuju integrate ceph-mon:client glance:ceph\n</code></pre>"},{"location":"roteiro3/main/#215-cinder","title":"2.15 Cinder","text":"<p>Criamos cinder.yaml (block-device: None) e implantamos o Cinder em cont\u00eainer LXD do server4:</p> <pre><code>juju deploy --to lxd:1 --channel yoga/stable --config cinder.yaml cinder\n\njuju deploy --channel 8.0/stable mysql-router cinder-mysql-router\njuju integrate cinder-mysql-router:db-router  mysql-innodb-cluster:db-router\njuju integrate cinder-mysql-router:shared-db cinder:shared-db\n\njuju integrate cinder:cinder-volume-service nova-cloud-controller:cinder-volume-service\njuju integrate cinder:identity-service       keystone:identity-service\njuju integrate cinder:amqp                   rabbitmq-server:amqp\njuju integrate cinder:image-service          glance:image-service\njuju integrate cinder:certificates           vault:certificates\n</code></pre> <p>Para usar Ceph como backend, adicionamos o charm subordinado:</p> <pre><code>juju deploy --channel yoga/stable cinder-ceph\njuju integrate cinder-ceph:storage-backend cinder:storage-backend\njuju integrate cinder-ceph:ceph            ceph-mon:client\njuju integrate cinder-ceph:ceph-access     nova-compute:ceph-access\n</code></pre>"},{"location":"roteiro3/main/#216-ceph-rados-gateway","title":"2.16 Ceph RADOS Gateway","text":"<p>Implantamos o Ceph RGW (gateway S3/Swift) em cont\u00eainer LXD do server3:</p> <pre><code>juju deploy --to lxd:0 --channel quincy/stable ceph-radosgw\njuju integrate ceph-radosgw:mon ceph-mon:radosgw\n</code></pre>"},{"location":"roteiro3/main/#217-configuracao-final-dos-discos-ceph-osd","title":"2.17 Configura\u00e7\u00e3o final dos discos Ceph OSD","text":"<p>Depois de validar todos os servi\u00e7os, aplicamos a configura\u00e7\u00e3o definitiva dos discos OSD:</p> <pre><code>juju config ceph-osd osd-devices='/dev/sdb'\n</code></pre>"},{"location":"roteiro3/main/#218-acerca-do-fim-da-infra","title":"2.18 Acerca do Fim da Infra","text":"<p>Monitoramos continuamente e avan\u00e7amos apenas quando todos os units ficaram em active/idle sem erros. Assim, conclu\u00edmos a implanta\u00e7\u00e3o do OpenStack via Juju + MAAS; a nuvem ficou pronta para criar redes, imagens e usu\u00e1rios finais.</p>"},{"location":"roteiro3/main/#3-setup","title":"3. Setup","text":"<p>Foi necess\u00e1rio configurar o Openstack, em destaque os servi\u00e7os que controlam:</p> <ul> <li> <p>As VMs (Nova)</p> </li> <li> <p>Os volumes de disco (Cinder)</p> </li> <li> <p>A estrutura de rede virtual (Neutron)</p> </li> </ul>"},{"location":"roteiro3/main/#31-autenticacao-keystone","title":"3.1 Autentica\u00e7\u00e3o (Keystone)","text":"<ol> <li> <p>Geramos e baixamos o arquivo openrc contendo OS_AUTH_URL, OS_PROJECT_NAME, OS_USERNAME, OS_PASSWORD, OS_USER_DOMAIN_NAME, OS_PROJECT_DOMAIN_NAME e OS_REGION_NAME.</p> </li> <li> <p>Carregamos as vari\u00e1veis no shell:</p> </li> </ol> <pre><code>source openrc\n</code></pre> <ol> <li>Confirmamos o acesso listando os servi\u00e7os:</li> </ol> <pre><code>openstack service list\n</code></pre>"},{"location":"roteiro3/main/#32-acompanhamento-em-tempo-real-horizon","title":"3.2 Acompanhamento em tempo real (Horizon)","text":"<p>Abrimos o Horizon em http:///horizon, dom\u00ednio admin_domain, usu\u00e1rio admin, e o deixamos ativo para visualizar as mudan\u00e7as."},{"location":"roteiro3/main/#tarefa-1","title":"Tarefa - 1","text":"<p>Prints iniciais:</p>"},{"location":"roteiro3/main/#a-status-do-juju","title":"(a) Status do Juju","text":""},{"location":"roteiro3/main/#b-dashboard-do-maas-com-as-maquinas","title":"(b) Dashboard do MAAS com as m\u00e1quinas","text":""},{"location":"roteiro3/main/#c-aba-overview","title":"(c) Aba overview","text":""},{"location":"roteiro3/main/#d-aba-instances","title":"(d) Aba instances","text":""},{"location":"roteiro3/main/#e-aba-topology","title":"(e) Aba topology","text":""},{"location":"roteiro3/main/#33-imagens-e-flavors","title":"3.3 Imagens e Flavors","text":"<ol> <li> <p>Instalamos o OpenStack client via snap.</p> </li> <li> <p>Habilitamos DNS interno no Neutron:</p> </li> </ol> <pre><code>juju config neutron-api enable-ml2-dns=\"true\"\njuju config neutron-api-plugin-ovn dns-servers=\"172.16.0.1\"\n</code></pre> <ol> <li> <p>Importamos a imagem Ubuntu 22.04 (Jammy) no Glance.</p> </li> <li> <p>Criamos quatro flavors (sem disco ef\u00eamero).</p> </li> </ol>"},{"location":"roteiro3/main/#34-rede-externa","title":"3.4 Rede externa","text":"<p>Definimos a rede public (flat, provider physnet1) com faixa 172.16.7.0 \u2013 172.16.8.255, gateway em 172.16.7.1 e pool de floating IPs reservado.</p>"},{"location":"roteiro3/main/#35-rede-interna-roteador","title":"3.5 Rede interna &amp; roteador","text":"<p>Criamos a rede private (192.169.0.0/24, sem DNS) e Adicionamos um roteador ligando private \u00e0 rede public e habilitamos NAT.</p>"},{"location":"roteiro3/main/#36-seguranca-e-chaves","title":"3.6 Seguran\u00e7a e chaves","text":"<p>Importamos o par de chaves SSH (~/.ssh/id_rsa.pub) como key-pair padr\u00e3o e Ajustamos o security group default para permitir SSH (22) e ICMP de qualquer origem.</p>"},{"location":"roteiro3/main/#37-instancia-de-validacao","title":"3.7 Inst\u00e2ncia de valida\u00e7\u00e3o","text":"<ol> <li> <p>Lan\u00e7amos a VM client (flavor m1.tiny, imagem Jammy, rede private, sem volume novo).</p> </li> <li> <p>Associamos um floating IP.</p> </li> <li> <p>Testamos ping e SSH a partir do main; conex\u00e3o bem-sucedida confirmou rotas, DHCP e DNS internos.</p> </li> </ol>"},{"location":"roteiro3/main/#38-escalonamento-dos-nos","title":"3.8 Escalonamento dos n\u00f3s","text":"<ol> <li>Liberamos (release) o server2 no MAAS e, em seguida, adicionamos:</li> </ol> <pre><code>juju add-unit nova-compute          # novo n\u00f3 de computa\u00e7\u00e3o  \njuju add-unit --to &lt;machine-id&gt; ceph-osd   # backend extra de storage  \n</code></pre> <ol> <li>Verificamos no Horizon que o hipervisor e o OSD extra apareceram dispon\u00edveis.</li> </ol>"},{"location":"roteiro3/main/#tarefa-2","title":"Tarefa 2","text":""},{"location":"roteiro3/main/#1-prints-pos-setup","title":"1. Prints P\u00f3s-Setup","text":""},{"location":"roteiro3/main/#a-dashboard-maas-com-as-maquinas","title":"(a) Dashboard MAAS com as m\u00e1quinas","text":""},{"location":"roteiro3/main/#b-aba-overview","title":"(b) Aba Overview","text":""},{"location":"roteiro3/main/#c-aba-instances","title":"(c) Aba instances","text":""},{"location":"roteiro3/main/#d-aba-topology","title":"(d) Aba topology","text":""},{"location":"roteiro3/main/#2-diferencas-encontradas-entre-os-prints-das-telas-na-tarefa-1-e-na-tarefa-2","title":"2. Diferen\u00e7as encontradas entre os prints das telas na Tarefa 1 e na Tarefa 2","text":"<ol> <li> <p>Dashboard do MAAS \u2013 na primeira captura, o server2 aparecia apenas como \u201cAllocated/Ready\u201d, ainda sem sistema instalado. Depois do setup ele surgiu como \u201cDeployed\u201d, indicando que foi instalado e passou a integrar o cluster.</p> </li> <li> <p>Compute \u203a Overview (Horizon) \u2013 inicialmente o painel listava tr\u00eas hipervisores (server3, 4 e 5) com zero vCPU e mem\u00f3ria em uso. Ap\u00f3s as configura\u00e7\u00f5es, um quarto hipervisor (server2) aparece e j\u00e1 se nota consumo de 1 vCPU e 1 GB de RAM, refletindo a execu\u00e7\u00e3o da VM de teste.</p> </li> <li> <p>Compute \u203a Instances (Horizon) \u2013 antes n\u00e3o havia nenhuma inst\u00e2ncia; agora existe a VM chamada client, estado Active, lan\u00e7ada no flavor m1.tiny, conectada \u00e0 rede private e com um floating IP associado.</p> </li> <li> <p>Network \u203a Topology (Horizon) \u2013 a topologia inicial exibia apenas a rede provider (physnet1). Depois do setup vimos a rede externa public, a rede interna private, o roteador router1 unindo as duas sub-redes e a porta da inst\u00e2ncia client, confirmando que o plano de rede virtual foi constru\u00eddo corretamente.</p> </li> </ol>"},{"location":"roteiro3/main/#3-origem-de-cada-novo-recurso","title":"3. Origem de cada novo recurso","text":"<ol> <li> <p>Hiper\u00advisor e OSD no server2 \u2013 liberamos o n\u00f3 reserva no MAAS, depois executamos juju add-unit nova-compute para transform\u00e1-lo em n\u00f3 de computa\u00e7\u00e3o e juju add-unit --to  ceph-osd para acrescentar um OSD de armazenamento. <li> <p>Imagem Ubuntu Jammy \u2013 baixamos a QCOW2 oficial e a carregamos no Glance via openstack image create, definindo formato qcow2 e nome \u201cubuntu-jammy\u201d.</p> </li> <li> <p>Flavors m1.tiny, m1.small, m1.medium e m1.large \u2013 criados com openstack flavor create, especificando vCPU, RAM e disco conforme a planilha do roteiro, todos sem disco ef\u00eamero.</p> </li> <li> <p>Rede externa public \u2013 constru\u00edda com openstack network create usando tipo flat em physnet1; em seguida openstack subnet create definiu a faixa 172.16.7.0/23 (pool de 172.16.7.10 a 172.16.8.250) e gateway 172.16.7.1.</p> </li> <li> <p>Rede interna private \u2013 criada via openstack network create private e openstack subnet create com o CIDR 192.169.0.0/24, sem DNS.</p> </li> <li> <p>Roteador router1 \u2013 gerado com openstack router create, configurado com gateway externo (public) e adicionado \u00e0 subnet private para habilitar NAT.</p> </li> <li> <p>Key-pair principal \u2013 importado no CLI com openstack keypair create --public-key ~/.ssh/id_rsa.pub main-key, permitindo login por SSH nas VMs.</p> </li> <li> <p>Regras de seguran\u00e7a \u2013 no Horizon, editamos o security group default para liberar tr\u00e1fego SSH (porta 22) e ICMP (ping) para qualquer origem.</p> </li> <li> <p>Inst\u00e2ncia client \u2013 lan\u00e7ada pelo Horizon, selecionando imagem Jammy, flavor m1.tiny, rede private e a key main-key.</p> </li> <li> <p>Floating IP \u2013 alocado do pool da rede public e associado \u00e0 VM pelo menu Associate Floating IP no Horizon.</p> </li>"},{"location":"roteiro3/main/#39-openstack-funcional","title":"3.9 Openstack Funcional","text":"<p>Com o OpenStack funcional, planejamos:</p> <ol> <li> <p>Automatizar a cria\u00e7\u00e3o de projetos e quotas para usu\u00e1rios finais.</p> </li> <li> <p>Publicar imagens customizadas (Docker-ready, GPU etc.).</p> </li> <li> <p>Documentar a pol\u00edtica de aloca\u00e7\u00e3o de floating IPs e monitorar uso via Grafana/Prometheus j\u00e1 existentes no cluster.</p> </li> </ol>"},{"location":"roteiro3/main/#tarefa-3","title":"Tarefa 3","text":"<pre><code>                       +--------------------+\n                       |      INSPER        |\n                       +---------+----------+\n                                 |\n                                 v    (link externo)\n                       +---------+----------+\n                       |  Roteador TP-Link  |\n                       +---------+----------+\n                                 |\n                                 v    (cabo LAN)\n                       +---------+----------+\n                       | Switch D-Link 28p  |\n                       +---------+----------+\n        __________________|________|________________________\n       |        |          |        |        |        |      |\n       v        v          v        v        v        v      v\n  +--------+ +--------+ +--------+ +--------+ +--------+ +--------+\n  |  Main  | |Server1 | |Server2 | |Server3 | |Server4 | |Server5 |\n  | (MAAS  | |Controller|Compute | |Compute | |Compute | |Compute |\n  | + Juju)| |  + API  |+OSD+VMs| |+OSD+VMs| |+OSD+VMs| |+OSD+VMs|\n  +--+--+--+ +----+---+ +----+---+ +----+---+ +----+---+ +----+---+\n     | br-ex     | br-ex   | br-ex   | br-ex   | br-ex   | br-ex\n     |           |         |         |         |         |\n     |           |         |         |         |         |\n     |      +----+---------+-----------------------------------+\n     |      |                    PublicNet (172.16.7.0/23)     |\n     |      +----+---------------------------------------------+\n     |           |\n     |           |  *Floating IP pool*\n     |           v\n     |      +----+----+\n     |      | Router1 |  (Neutron L3, NAT)\n     |      +----+----+\n     |           |\n     |           |  *interface interna*\n     v           v\n</code></pre> <p>+-----------------------------+ |   PrivateNet (192.169.0.0)  | +--------------+--------------+                |                v          +-----+-----+          |  VM client|   (m1.tiny, key-pair main-key)          +-----------+</p>"},{"location":"roteiro3/main/#4-app","title":"4. App","text":"<p>Nesta etapa levantamos as aplica\u00e7\u00f5es do projeto em quatro VMs na nuvem OpenStack:</p> <ul> <li> <p>2 \u00d7 API (FastAPI, etapa 1)</p> </li> <li> <p>1 \u00d7 Banco de dados (PostgreSQL, etapa 1)</p> </li> <li> <p>1 \u00d7 Load-Balancer (Nginx)</p> </li> </ul> <p>Todas as inst\u00e2ncias foram criadas dentro da rede PrivateNet (192.169.0.0/24) e expostas ao mundo por meio de um floating IP na rede PublicNet (172.16.0.0/20).</p>"},{"location":"roteiro3/main/#tarefa-4","title":"Tarefa 4","text":""},{"location":"roteiro3/main/#lista-das-maquinas-virtuais","title":"Lista das M\u00e1quinas Virtuais","text":""},{"location":"roteiro3/main/#servers-alocados","title":"Servers Alocados","text":""},{"location":"roteiro4/main/","title":"Roteiro 4 - SLA, DR e IaC","text":""},{"location":"roteiro4/main/#1-objetivo","title":"1. Objetivo","text":"<p>Este roteiro tem como objetivo:</p> <ul> <li>Compreender os fundamentos de Infraestrutura como C\u00f3digo (IaC)</li> <li>Entender conceitos de SLA (Service Level Agreement) e DR (Disaster Recovery)</li> <li>Implementar uma hierarquia de projetos no OpenStack</li> <li>Criar infraestrutura via Terraform e implantar aplica\u00e7\u00f5es de forma isolada</li> </ul>"},{"location":"roteiro4/main/#2-iac-infraestrutura-como-codigo","title":"2. IaC - Infraestrutura como C\u00f3digo","text":""},{"location":"roteiro4/main/#21-o-que-e-iac","title":"2.1 O que \u00e9 IaC?","text":"<p>IaC (Infraestrutura como C\u00f3digo) \u00e9 uma abordagem que permite gerenciar e provisionar ambientes de TI por meio de arquivos de configura\u00e7\u00e3o, evitando a configura\u00e7\u00e3o manual e promovendo consist\u00eancia e reprodutibilidade.</p> <p>Principais benef\u00edcios:</p> <ul> <li>Ambientes consistentes e reprodut\u00edveis</li> <li>Automatiza\u00e7\u00e3o de configura\u00e7\u00f5es</li> <li>Documenta\u00e7\u00e3o via c\u00f3digo</li> <li>Controle de vers\u00e3o das mudan\u00e7as de infraestrutura</li> </ul>"},{"location":"roteiro4/main/#22-ferramentas-terraform","title":"2.2 Ferramentas: Terraform","text":"<p>O Terraform, da HashiCorp, \u00e9 uma das ferramentas mais usadas para IaC. Ele permite definir recursos de infraestrutura em arquivos leg\u00edveis e gerenciar o ciclo de vida completo do ambiente.</p> <p>Passos t\u00edpicos de uso:</p> <pre><code>terraform init     # Inicializa os plugins\nterraform plan     # Mostra o que ser\u00e1 alterado\nterraform apply    # Aplica as mudan\u00e7as\n</code></pre>"},{"location":"roteiro4/main/#23-instalacao-do-terraform-no-ubuntudebian","title":"2.3 Instala\u00e7\u00e3o do Terraform no Ubuntu/Debian","text":"<pre><code>wget -O- https://apt.releases.hashicorp.com/gpg | gpg --dearmor | sudo tee /usr/share/keyrings/hashicorp-archive-keyring.gpg\ngpg --no-default-keyring --keyring /usr/share/keyrings/hashicorp-archive-keyring.gpg --fingerprint\necho \"deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/hashicorp.list\nsudo apt update &amp;&amp; sudo apt install terraform\n</code></pre>"},{"location":"roteiro4/main/#3-openstack-hierarquia-de-projetos","title":"3. OpenStack - Hierarquia de Projetos","text":""},{"location":"roteiro4/main/#31-criacao-de-dominio-e-projetos","title":"3.1 Cria\u00e7\u00e3o de Dom\u00ednio e Projetos","text":"<ul> <li>Acessando: <code>Identity &gt; Domains</code></li> <li>Foi criado o dom\u00ednio \"AlunosDomain2\"</li> <li>Acessando: <code>Identity &gt; Projects</code></li> <li>Criou-se projetos: <code>KitTRafael</code> e <code>KitTPedro</code></li> </ul>"},{"location":"roteiro4/main/#32-criacao-de-usuarios","title":"3.2 Cria\u00e7\u00e3o de Usu\u00e1rios","text":"<ul> <li>Acessando: <code>Identity &gt; Users</code></li> <li>Criou-se usu\u00e1rios <code>rafael</code> e <code>pedro</code></li> <li>Associou-se cada um ao seu projeto</li> <li>Atribuiu-se o papel de admin aos usu\u00e1rios</li> </ul>"},{"location":"roteiro4/main/#33-prints-de-validacao","title":"3.3 Prints de Valida\u00e7\u00e3o","text":"<p>1. Projects</p> <p></p> <p>2. Users</p> <p></p> <p>3. Compute Overview</p> <p></p> <p>4. Compute Instances</p> <p></p> <p>5. Network Topology</p> <p></p> <p>1. Projects</p> <p></p> <p>2. Compute Overview</p> <p></p> <p>3. Compute Instances</p> <p></p> <p>4. Network Topology</p>"},{"location":"roteiro4/main/#_1","title":"Roteiro 4","text":""},{"location":"roteiro4/main/#4-infraestrutura-com-terraform","title":"4. Infraestrutura com Terraform","text":""},{"location":"roteiro4/main/#41-estrutura-de-pastas","title":"4.1 Estrutura de Pastas","text":"<pre><code>/KiTNome/\n\u2514\u2500\u2500 terraform/\n    \u251c\u2500\u2500 provider.tf\n    \u251c\u2500\u2500 instance1.tf\n    \u251c\u2500\u2500 instance2.tf\n    \u251c\u2500\u2500 network.tf\n    \u2514\u2500\u2500 router.tf\n</code></pre>"},{"location":"roteiro4/main/#42-arquivos-terraform","title":"4.2 Arquivos Terraform","text":"<p>provider.tf</p> <pre><code>provider \"openstack\" {\n  region    = \"RegionOne\"\n  user_name = \"SEU_USUARIO\"\n}\n</code></pre> <p>instance1.tf</p> <pre><code>resource \"openstack_compute_instance_v2\" \"instancia\" {\n  name        = \"basic\"\n  image_name  = \"bionic-amd64\"\n  flavor_name = \"m1.small\"\n  key_pair    = \"mykey\"\n  network {\n    name = \"network_1\"\n  }\n}\n</code></pre> <p>instance2.tf</p> <pre><code>resource \"openstack_compute_instance_v2\" \"instancia\" {\n  name        = \"basic2\"\n  image_name  = \"jammy-amd64\"\n  flavor_name = \"m1.tiny\"\n  key_pair    = \"mykey\"\n  network {\n    name = \"network_1\"\n  }\n}\n</code></pre> <p>network.tf</p> <pre><code>resource \"openstack_networking_network_v2\" \"network_1\" {\n  name = \"network_1\"\n}\n\nresource \"openstack_networking_subnet_v2\" \"subnet_1\" {\n  network_id = openstack_networking_network_v2.network_1.id\n  cidr       = \"192.167.199.0/24\"\n}\n</code></pre> <p>router.tf</p> <pre><code>resource \"openstack_networking_router_v2\" \"router_1\" {\n  name                = \"my_router\"\n  external_network_id = \"&lt;ID_EXT_NETWORK&gt;\"\n}\n\nresource \"openstack_networking_router_interface_v2\" \"int_1\" {\n  router_id = openstack_networking_router_v2.router_1.id\n  subnet_id = openstack_networking_subnet_v2.subnet_1.id\n}\n</code></pre>"},{"location":"roteiro4/main/#43-execucao","title":"4.3 Execu\u00e7\u00e3o","text":"<pre><code>source openstack-base/openrc\nsource arquivo.sh\nterraform init\nterraform plan\nterraform apply\n</code></pre>"},{"location":"roteiro4/main/#5-sla-e-disaster-recovery","title":"5. SLA e Disaster Recovery","text":""},{"location":"roteiro4/main/#51-escolha-da-nuvem","title":"5.1 Escolha da Nuvem","text":"<p>Voc\u00ea escolheria Public Cloud ou Private Cloud? Justifique.</p> <p>Optaria por uma Private Cloud, pois o sistema \u00e9 cr\u00edtico, com dados sigilosos e necessidade de controle sobre seguran\u00e7a e compliance. A Private Cloud oferece maior controle sobre as configura\u00e7\u00f5es de rede, pol\u00edticas de acesso e backups, al\u00e9m de permitir personaliza\u00e7\u00e3o de acordo com as demandas espec\u00edficas da empresa. Ainda assim, buscaria implementar uma arquitetura h\u00edbrida caso haja a necessidade de escalabilidade em momentos de pico.</p>"},{"location":"roteiro4/main/#52-justificativa-do-time-devops","title":"5.2 Justificativa do Time DevOps","text":"<p>O time de DevOps \u00e9 essencial para garantir a integra\u00e7\u00e3o cont\u00ednua, entrega automatizada e monitoramento constante da infraestrutura e aplica\u00e7\u00f5es. Eles promovem agilidade, reduzem falhas manuais, aumentam a confiabilidade dos sistemas e permitem r\u00e1pida recupera\u00e7\u00e3o diante de incidentes. Para um sistema cr\u00edtico, a atua\u00e7\u00e3o de DevOps \u00e9 estrat\u00e9gica, pois permite responder com efici\u00eancia a mudan\u00e7as e falhas operacionais.</p>"},{"location":"roteiro4/main/#53-plano-de-dr-e-ha","title":"5.3 Plano de DR e HA","text":"<p>Amea\u00e7as Identificadas:</p> <ul> <li>Falha de hardware</li> <li>Ataques cibern\u00e9ticos (ransomware, DDoS)</li> <li>Interrup\u00e7\u00f5es de energia/rede</li> <li>Erros humanos em configura\u00e7\u00f5es e deploys</li> </ul> <p>A\u00e7\u00f5es para recupera\u00e7\u00e3o:</p> <ul> <li>Configura\u00e7\u00e3o de snapshots di\u00e1rios dos volumes</li> <li>Scripts de recupera\u00e7\u00e3o r\u00e1pida em Terraform</li> <li>Rede redundante com failover autom\u00e1tico</li> <li>Execu\u00e7\u00e3o de simula\u00e7\u00f5es de desastre regularmente</li> </ul> <p>Pol\u00edtica de Backup:</p> <ul> <li>Backups di\u00e1rios criptografados</li> <li>Reten\u00e7\u00e3o m\u00ednima de 30 dias</li> <li>Armazenamento redundante (on-premise + nuvem)</li> <li>Verifica\u00e7\u00f5es peri\u00f3dicas de integridade dos backups</li> </ul> <p>Alta Disponibilidade (HA):</p> <ul> <li>Servi\u00e7os replicados em m\u00faltiplos n\u00f3s</li> <li>Load balancer com health check</li> <li>Monitoramento 24/7 com alertas</li> <li>Auto scaling em caso de picos de uso</li> </ul> <p>Plano de DR e HA focado em recupera\u00e7\u00e3o \u00e1gil e mitiga\u00e7\u00e3o de riscos em sistemas cr\u00edticos. A implementa\u00e7\u00e3o deve ser constantemente testada e revisada para manter a resili\u00eancia do ambiente.</p>"}]}