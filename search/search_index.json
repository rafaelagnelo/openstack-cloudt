{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Computa\u00e7\u00e3o em Nuvem","text":"Edi\u00e7\u00e3o <p>2025.1</p>"},{"location":"#kit-t","title":"KIT-T","text":"<p>Pedro Henrique Vidal</p> <p>Rafael Agnelo</p>"},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Roteiro 1 - Data 23/02/2025</li> <li> Roteiro 2</li> <li> Roteiro 3</li> <li> Roteiro 4</li> <li> Projeto</li> </ul>"},{"location":"#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"roteiro1/main/","title":"Roteiro 1 - MAAS","text":""},{"location":"roteiro1/main/#1-objetivo","title":"1. Objetivo","text":"<p>O presente relat\u00f3rio tem como alvo abstrair os seguintes conhecimentos:</p> <p>\u2022   Entendimento de conceitos b\u00e1sicos sobre gerenciamento de hardware (Bare Metal) e MaaS (Metal as a Service).</p> <p>\u2022   Entendimento de conceitos b\u00e1sicos sobre redes de computadores.</p>"},{"location":"roteiro1/main/#2-infraestrutura","title":"2. Infraestrutura","text":"<p>O kit utilizado para esta atividade foi composto pelos seguintes itens:</p> <ul> <li> <p>1 NUC (main) com 10 GB de RAM e 1 SSD de 120 GB.</p> </li> <li> <p>1 NUC (server1) com 12 GB de RAM e 1 SSD de 120 GB.</p> </li> <li> <p>1 NUC (server2) com 16 GB de RAM e 2 SSDs de 120 GB.</p> </li> <li> <p>3 NUCs (server3, server4, server5) com 32 GB de RAM e 2 SSDs de 120 GB.</p> </li> <li> <p>1 Switch D-Link DSG-1210-28 com 28 portas.</p> </li> <li> <p>1 Roteador TP-Link TL-R470T+.</p> </li> </ul> <p>Para a reliza\u00e7\u00e3o desse roteiro contamos com um ponto de rede (cabo) pr\u00f3prio conectado \u00e0 rede do Insper e um IP de entrada configurado diretamente no roteador.</p>"},{"location":"roteiro1/main/#21-instalacao-do-ubuntu-server-na-maquina-principal-main","title":"2.1 Instala\u00e7\u00e3o do Ubuntu Server na m\u00e1quina principal (main)","text":"<p>A primeira etapa da configura\u00e7\u00e3o envolveu a instala\u00e7\u00e3o do sistema operacional Ubuntu Server 22.04 LTS na m\u00e1quina principal (NUC main). O procedimento foi realizado via pendrive boot\u00e1vel, configurado previamente com a imagem ISO do Ubuntu.</p> <p>As defini\u00e7\u00f5es aplicadas durante a instala\u00e7\u00e3o foram: Instala\u00e7\u00e3o do Ubuntu:</p> <p>Hostname: main</p> <p>Usu\u00e1rio: cloud</p> <p>Senha: cloudt</p>"},{"location":"roteiro1/main/#22-instalacao-do-maas","title":"2.2 Instala\u00e7\u00e3o do MAAS","text":"<p>Com o sistema operacional instalado, foi realizada a instala\u00e7\u00e3o do MAAS (Metal as a Service) na vers\u00e3o est\u00e1vel 3.5.3. Antes disso, foram feitos testes de conectividade com os comandos ping para verificar roteamento e resolu\u00e7\u00e3o de DNS.</p> <p>Com a conectividade confirmada, os seguintes comandos foram executados:</p> sudo apt update &amp;&amp; sudo apt upgrade -ysudo snap install maas --channel=3.5/stablesudo snap install maas-test-db"},{"location":"roteiro1/main/#23-inicializacao-do-maas-e-criacao-do-administrador","title":"2.3 Inicializa\u00e7\u00e3o do MAAS e cria\u00e7\u00e3o do administrador","text":"<p>A inicializa\u00e7\u00e3o do MAAS foi feita com os seguintes comandos:</p> <p><pre><code>$ sudo maas init region+rack --maas-url http://172.16.0.3:5240/MAAS --database-uri maas-test-db:///\n$ sudo maas createadmin\n</code></pre> Durante a cria\u00e7\u00e3o do administrador, utilizamos o login cloud, a senha padr\u00e3o da disciplina e deixamos o campo de chave SSH vazio.</p> <p>Depois, geramos um par de chaves SSH com senha vazia:</p> <pre><code>$ ssh-keygen -t rsa\n$ cat ./.ssh/id_rsa.pub\n</code></pre> <p>A chave p\u00fablica gerada foi copiada e registrada no dashboard do MAAS.</p>"},{"location":"roteiro1/main/#24-configuracao-inicial-do-dashboard-do-maas-e-configuracao-do-dhcp","title":"2.4 Configura\u00e7\u00e3o inicial do Dashboard do MAAS e configura\u00e7\u00e3o do DHCP","text":"<p>Acessamos o dashboard do MAAS via navegador em http://172.16.0.3:5240/MAAS.</p> <p>As configura\u00e7\u00f5es aplicadas nessa etapa inclu\u00edram: Upload da chave SSH gerada; Defini\u00e7\u00e3o do DNS Forwarder para 172.20.129.131; Importa\u00e7\u00e3o das imagens do Ubuntu 22.04 LTS e 20.04 LTS; Atualiza\u00e7\u00e3o do par\u00e2metro global net.ifnames=0 em Settings &gt; General.</p> <p>Tamb\u00e9m, foi habilitado o servi\u00e7o de DHCP diretamente no MAAS Controller, com os seguintes ajustes:</p> <ul> <li> <p>Faixa reservada: de 172.16.11.1 at\u00e9 172.16.14.255</p> </li> <li> <p>DNS da subrede: 172.20.129.131</p> </li> </ul> <p>Obs:</p> <p>A integridade do ambiente foi confirmada por meio do painel de controladores do MAAS, onde todos os servi\u00e7os essenciais, incluindo regiond, rackd e dhcpd, apresentaram status verde.</p>"},{"location":"roteiro1/main/#25-comissionamento-das-maquinas","title":"2.5 Comissionamento das m\u00e1quinas","text":"<p>As NUCs server1 a server5 foram registradas como hosts no MAAS. Todas as m\u00e1quinas realizaram o boot via PXE e foram comissionadas com sucesso. Ap\u00f3s o processo, passaram a apresentar status Ready, com todas as informa\u00e7\u00f5es de hardware corretamente detectadas.</p>"},{"location":"roteiro1/main/#26-criacao-da-ovs-bridge-br-ex-e-configuracao-do-nat","title":"2.6 Cria\u00e7\u00e3o da OVS bridge (br-ex) e configura\u00e7\u00e3o do NAT","text":"<p>Foi criada uma Open vSwitch bridge (OVS) chamada br-ex em cada n\u00f3 de nuvem, utilizando a interface f\u00edsica enp1s0. Essa ponte foi configurada no painel do MAAS, na aba Network de cada m\u00e1quina. A configura\u00e7\u00e3o de br-ex \u00e9 essencial para o funcionamento futuro do OVN e para suportar redes overlay sem exigir m\u00faltiplas interfaces f\u00edsicas por n\u00f3.</p> <p>Finalmente, configuramos um NAT no roteador do kit, liberando acesso externo \u00e0 m\u00e1quina principal (main) pela porta 22. Isso permitiu o uso de SSH mesmo fora da rede local.</p> <p>Tamb\u00e9m foi criada uma regra de gest\u00e3o para o endere\u00e7o 0.0.0.0/0, permitindo acesso remoto \u00e0 interface de gerenciamento do pr\u00f3prio roteador.</p>"},{"location":"roteiro1/main/#3-aplicacao","title":"3. Aplica\u00e7\u00e3o","text":"<p>Fizemos a realiza\u00e7\u00e3o de um deploy manual para uma aplica\u00e7\u00e3o simples na nuvem MaaS da nossa dupla.</p>"},{"location":"roteiro1/main/#31-ajuste-de-dns-para-deploy-bare-metal","title":"3.1 Ajuste de DNS para deploy Bare Metal","text":"<p>Ajustamos o DNS da nossa rede bare-metal diretamente pelo MAAS. Na aba Subnets, acessamos a subnet 172.16.0.0/20, editamos o campo Subnet summary e substitu\u00edmos o DNS para o do Insper: 172.20.129.131.</p>"},{"location":"roteiro1/main/#32-deploy-manual-do-banco-de-dados-postgresql-no-server1","title":"3.2 Deploy manual do banco de dados PostgreSQL no server1","text":"<p>A partir do dashboard do MAAS, realizamos o deploy do Ubuntu 22.04 no server1. Ap\u00f3s o deploy, acessamos o terminal via SSH:</p> <pre><code>$ ssh cloud@172.16.15.1\n</code></pre> <p>No terminal, executamos a instala\u00e7\u00e3o do PostgreSQL:</p> <pre><code>$ sudo apt update\n$ sudo apt install postgresql postgresql-contrib -y\n</code></pre> <p>Em seguida, configuramos o banco:</p> <pre><code>$ sudo su - postgres\n$ createuser -s cloud -W          \n$ createdb -O cloud tasks\n</code></pre> <p>Alteramos os arquivos de configura\u00e7\u00e3o para permitir conex\u00f5es externas:</p> <pre><code>$ nano /etc/postgresql/14/main/postgresql.conf\n# linha: listen_addresses = '*'\n\n$ nano /etc/postgresql/14/main/pg_hba.conf\n# linha adicionada: host all all 172.16.0.0/20 trust\n</code></pre> <p>Por fim, reiniciamos o servi\u00e7o e liberamos a porta padr\u00e3o do PostgreSQL:</p> <pre><code>$ sudo ufw allow 5432/tcp\n$ sudo systemctl restart postgresql\n</code></pre>"},{"location":"roteiro1/main/#tarefa-1-validacao-do-banco-de-dados-postgresql","title":"Tarefa 1 - Valida\u00e7\u00e3o do banco de dados PostgreSQL","text":"<ol> <li>Verifica\u00e7\u00e3o do status do PostgreSQL no server1</li> </ol> <p>O comando sudo systemctl status postgresql foi executado no terminal da m\u00e1quina server1. A sa\u00edda mostra que o servi\u00e7o est\u00e1 com o status active (exited), indicando que o PostgreSQL est\u00e1 corretamente habilitado e funcionando no sistema operacional. Isso comprova que o banco de dados foi inicializado com sucesso e est\u00e1 pronto para receber conex\u00f5es.</p> <p></p> <ol> <li>Teste de conex\u00e3o local com o banco de dados PostgreSQL</li> </ol> <p>Na imagem, foi utilizado o comando psql -U cloud -h 172.16.15.0 tasks dentro do pr\u00f3prio server1 para acessar o banco de dados tasks. A autentica\u00e7\u00e3o foi feita com sucesso utilizando o usu\u00e1rio cloud, e a conex\u00e3o foi estabelecida diretamente com o IP da m\u00e1quina local. A presen\u00e7a do prompt tasks=# confirma que a conex\u00e3o est\u00e1 ativa e funcional.</p> <p></p> <ol> <li>Acess\u00edvel a partir da m\u00e1quina MAIN</li> </ol> <p>Mostra o processo de instala\u00e7\u00e3o do postgresql-client, que \u00e9 o cliente necess\u00e1rio para fazer conex\u00e3o externa ao banco de dados PostgreSQL, partindo da m\u00e1quina main.</p> <p></p>"},{"location":"roteiro1/main/#33-deploy-da-aplicacao-django-no-server2-via-maas-cli","title":"3.3 Deploy da aplica\u00e7\u00e3o Django no server2 via MAAS CLI","text":"<p>Acessamos o terminal do main e realizamos login no MAAS:</p> <pre><code>$ maas login cloud http://172.16.0.3:5240/MAAS/\n</code></pre> <p>Solicitamos a aloca\u00e7\u00e3o do server2:</p> <pre><code>$ maas cloud machines allocate name=server2\n</code></pre> <p>Com o system_id retornado, iniciamos o deploy:</p> <pre><code>$ maas cloud machine deploy [system_id]\n</code></pre> <p>Acessamos o server2 via SSH e iniciamos o processo de instala\u00e7\u00e3o da aplica\u00e7\u00e3o:</p> <pre><code>$ git clone https://github.com/raulikeda/tasks.git\n$ cd tasks\n$ ./install.sh\n$ sudo reboot\n</code></pre>"},{"location":"roteiro1/main/#34-acesso-externo-via-tunel-ssh","title":"3.4 Acesso externo via t\u00fanel SSH","text":"<p>Com o servi\u00e7o Django rodando na porta 8080 do server2, realizamos o acesso externo usando um t\u00fanel SSH a partir do main:</p> <pre><code>$ ssh cloud@10.103.0.X -L 8001:172.16.15.2:8080\n</code></pre> <p>Abrimos o navegador e acessamos:</p> <pre><code>http://localhost:8001/admin/\n</code></pre>"},{"location":"roteiro1/main/#tarefa-2","title":"Tarefa 2","text":"<ol> <li>Comprova\u00e7\u00e3o da Infraestrutura MaaS e Imagens</li> </ol> <p>A imagem exibe cinco m\u00e1quinas registradas no MAAS. Dentre elas, duas (server1 e server2) est\u00e3o no estado Deployed, com IPs vis\u00edveis (172.16.15.0 e 172.16.15.6), o que confirma que j\u00e1 passaram pelo processo de deploy com a imagem do Ubuntu 22.04 LTS. As demais (server3, server4 e server5) est\u00e3o em estado Ready, prontas para serem utilizadas.</p> <p></p> <ol> <li>Sincroniza\u00e7\u00e3o da imagem Ubuntu 22.04 LTS no MAAS</li> </ol> <p>A imagem da vers\u00e3o 22.04 LTS do Ubuntu (arquitetura amd64) foi devidamente baixada e sincronizada com sucesso, estando marcada como Synced. Isso confirma que a infraestrutura est\u00e1 pronta para realizar deploys com essa vers\u00e3o nas m\u00e1quinas dispon\u00edveis no pool do kit.</p> <p></p> <ol> <li>Comissionamento das m\u00e1quina com status \u201cPassed\u201d</li> </ol> <p>Agora, vemos os testes de comissionamento realizados nas 5 m\u00e1quinasTodos os testes \u2014 incluindo verifica\u00e7\u00e3o de interfaces de rede, informa\u00e7\u00f5es de hardware, portas seriais e hints de configura\u00e7\u00e3o \u2014 retornaram com status \u201cPassed\u201d, o que garante que o n\u00f3 foi corretamente detectado e est\u00e1 apto a ser utilizado na nuvem bare-metal. </p> <p>Server 1:</p> <p></p> <p>Server 2:</p> <p></p> <p>Server 3: </p> <p></p> <p>Server 4: </p> <p></p> <p>Server 5:</p> <p></p>"},{"location":"roteiro1/main/#35-deploy-automatizado-da-aplicacao-no-server3-com-ansible","title":"3.5 Deploy automatizado da aplica\u00e7\u00e3o no server3 com Ansible","text":"<p>Realizamos a instala\u00e7\u00e3o do Ansible no main:</p> <pre><code>$ sudo apt install ansible\n$ wget https://raw.githubusercontent.com/raulikeda/tasks/master/tasks-install-playbook.yaml\n</code></pre> <p>Solicitamos o deploy do server3 via MAAS CLI e rodamos o playbook:</p> <pre><code>$ maas cloud machines allocate name=server3\n$ maas cloud machine deploy [system_id]\n\n$ ansible-playbook tasks-install-playbook.yaml --extra-vars server=172.16.15.3\n</code></pre> <p>A aplica\u00e7\u00e3o foi implantada automaticamente, conectando-se ao banco do server1.</p>"},{"location":"roteiro1/main/#tarefa-3","title":"Tarefa 3","text":"<ol> <li>M\u00e1quinas alocadas no MAAS ap\u00f3s deploy da aplica\u00e7\u00e3o</li> </ol> <p>Vemos que tr\u00eas m\u00e1quinas (server1, server2 e server3) est\u00e3o com status Deployed, indicando que foram utilizadas para banco de dados e aplica\u00e7\u00f5es Django.</p> <p></p> <ol> <li>Aplica\u00e7\u00e3o Django em execu\u00e7\u00e3o no server2</li> </ol> <p>O print mostra o Hello, world. You're at the tasks index. retornado pela aplica\u00e7\u00e3o Django, acessada via navegador local utilizando localhost:8001/tasks. Isso confirma que a aplica\u00e7\u00e3o est\u00e1 funcional e acess\u00edvel.</p> <p></p> <ol> <li>Explica\u00e7\u00e3o do Deploy manual</li> </ol> <p>Para realizar o deploy manual da aplica\u00e7\u00e3o Django no server2, primeiramente solicitamos a aloca\u00e7\u00e3o da m\u00e1quina via MAAS CLI. Em seguida, executamos os comandos para clonar o reposit\u00f3rio da aplica\u00e7\u00e3o e iniciar o processo de instala\u00e7\u00e3o via o script install.sh. O servi\u00e7o foi configurado para escutar na porta 8080, e o acesso externo foi poss\u00edvel atrav\u00e9s da cria\u00e7\u00e3o de um t\u00fanel SSH a partir do main, redirecionando o tr\u00e1fego da porta 8001 local para a porta 8080 do server2. Esse procedimento permitiu validar o funcionamento completo da aplica\u00e7\u00e3o hospedada na m\u00e1quina provisionada.</p>"},{"location":"roteiro1/main/#36-configuracao-do-load-balancer-com-nginx-no-server4","title":"3.6 Configura\u00e7\u00e3o do Load Balancer com NGINX no server4","text":"<p>Instalamos o NGINX no server4:</p> <pre><code>$ sudo apt-get install nginx\n</code></pre> <p>Editamos o arquivo de configura\u00e7\u00e3o padr\u00e3o:</p> <pre><code>$ sudo nano /etc/nginx/sites-available/default\n</code></pre> <p>Adicionamos o m\u00f3dulo de balanceamento:</p> <p><pre><code>upstream backend {\n    server 172.16.15.2:8080;\n    server 172.16.15.3:8080;\n}\n\nserver {\n    listen 80;\n    location / {\n        proxy_pass http://backend;\n    }\n}\n</code></pre> Reiniciamos o servi\u00e7o:</p> <pre><code>$ sudo service nginx restart\n</code></pre> <p>Modificamos o conte\u00fado da fun\u00e7\u00e3o index() no arquivo tasks/views.py de cada servidor para exibir uma mensagem personalizada, identificando qual servidor respondeu a requisi\u00e7\u00e3o.</p>"},{"location":"roteiro1/main/#tarefa-4","title":"Tarefa 4","text":"<ol> <li>Acesso ao Server2 via t\u00fanel SSH</li> </ol> <p>A imagem mostra o terminal local conectado ao server2 via SSH. Este t\u00fanel permite o redirecionamento do tr\u00e1fego da porta 8001 do computador local para a porta 8080 do server2, onde a aplica\u00e7\u00e3o Django est\u00e1 hospedada. Vendo o texto padr\u00e3o no fundo, confirmamos o funcionamento da aplica\u00e7\u00e3o Django no Server 2.</p> <p></p> <ol> <li>Acesso ao Server3 via t\u00fanel SSH</li> </ol> <p>O mesmo racional do acesso ao server 2, mas para o server3.</p> <p></p> <ol> <li>Diferen\u00e7a entre instala\u00e7\u00e3o manual e via Ansible</li> </ol> <p>A instala\u00e7\u00e3o manual exige executar cada passo diretamente no terminal (como clonar o reposit\u00f3rio, rodar install.sh, configurar depend\u00eancias e reiniciar). J\u00e1 com o Ansible, tudo \u00e9 feito automaticamente por meio de um playbook, o que economiza tempo, reduz erros e garante que o processo seja id\u00eantico em v\u00e1rios servidores.</p>"},{"location":"roteiro1/main/#tarefa-5","title":"Tarefa 5","text":"<ol> <li>Quatro m\u00e1quinas no estado Deployed com IPs vis\u00edveis no MAAS</li> </ol> <p>Vemos o dashboard do MAAS com quatro m\u00e1quinas no estado Deployed (server1, server2, server3 e server4), cada uma com seu respectivo IP vis\u00edvel. Essa visualiza\u00e7\u00e3o confirma que os n\u00f3s est\u00e3o devidamente provisionados com Ubuntu 22.04 LTS e prontos para opera\u00e7\u00e3o, incluindo o server4, que atuar\u00e1 como balanceador de carga via NGINX.</p> <p></p> <ol> <li>GET request mostra resposta da aplica\u00e7\u00e3o no server3 e server 2 via proxy reverso</li> </ol> <p>Por fim, notamos o conte\u00fado acessado via localhost:8001/tasks/, que est\u00e1 sendo redirecionado atrav\u00e9s do proxy reverso configurado no server4 (NGINX). As mensagens \"Ol\u00e1, voc\u00ea est\u00e1 no server3.\" e \"Ol\u00e1, voc\u00ea est\u00e1 no server2.\" confirmam funcionamento do balanceamento de carga via NGINX. </p> <p>Server3: </p> <p></p> <p>Server2:</p> <p></p>"},{"location":"roteiro1/main/#37-finalizacao-e-limpeza-do-ambiente","title":"3.7 Finaliza\u00e7\u00e3o e limpeza do ambiente","text":"<p>Ap\u00f3s os testes, realizamos o release de todos os servidores do kit atrav\u00e9s do dashboard do MAAS. Esse processo liberou os recursos de hardware utilizados para as aplica\u00e7\u00f5es e preparou o ambiente para futuros testes.</p>"},{"location":"roteiro2/main/","title":"Roteiro 2 - Juju","text":""},{"location":"roteiro2/main/#1-objetivo","title":"1. Objetivo","text":"<p>Tendo compreendido o relat\u00f3rio 1, o presente relat\u00f3rio tem como alvo os seguintes objetivos com Deployment Orchestration:</p> <p>\u2022   Automatizar a infraestrutura, reduzindo a complexidade e garantindo consist\u00eancia nas implanta\u00e7\u00f5es.</p> <p>\u2022   Gerenciamento centralizado, permitindo administra\u00e7\u00e3o eficiente de m\u00faltiplos servidores.</p> <p>\u2022   Integra\u00e7\u00e3o com provedores, usando recursos f\u00edsicos de forma otimizada.</p>"},{"location":"roteiro2/main/#2-infraestrutura","title":"2. Infraestrutura","text":""},{"location":"roteiro2/main/#21-por-que-utilizar-o-juju-para-deployment-orchestration","title":"2.1 Por que utilizar o Juju para Deployment Orchestration?","text":"<p>O Juju \u00e9 uma ferramenta de orquestra\u00e7\u00e3o que atua desde o provisionamento at\u00e9 a configura\u00e7\u00e3o e integra\u00e7\u00e3o de servi\u00e7os. Ao contr\u00e1rio do Ansible, que se limita \u00e0 configura\u00e7\u00e3o de m\u00e1quinas j\u00e1 provisionadas, o Juju se integra diretamente com o MAAS (Metal as a Service), permitindo orquestrar o ambiente Bare Metal de forma completa e automatizada.</p>"},{"location":"roteiro2/main/#22-instalacao-do-juju","title":"2.2 Instala\u00e7\u00e3o do Juju","text":"<p>Inicialmente, foi necess\u00e1rio acessar o dashboard do MAAS e garantir que todas as m\u00e1quinas nomeadas como server1, server2, server3, server4 e server5 estivessem com status Ready. Realizamos o release das m\u00e1quinas que haviam sido anteriormente utilizadas para as aplica\u00e7\u00f5es Django e PostgreSQL, a fim de garantir que estivessem dispon\u00edveis para novas aloca\u00e7\u00f5es.</p> <p>Tendo as m\u00e1quinas prontas,  acessamos o n\u00f3 principal (main) via SSH para a instala\u00e7\u00e3o do Juju. A instala\u00e7\u00e3o foi realizada utilizando o Snap com o canal da vers\u00e3o 3.6:</p> <pre><code>$ sudo snap install juju --channel 3.6\n</code></pre>"},{"location":"roteiro2/main/#23-configuracao-da-cloud-maas-no-juju","title":"2.3 Configura\u00e7\u00e3o da Cloud MAAS no Juju","text":"<p>Ap\u00f3s a instala\u00e7\u00e3o do Juju, foi necess\u00e1rio garantir que o MAAS estivesse vis\u00edvel como provedor de cloud. Para isso, executamos o comando:</p> <pre><code>$ juju clouds\n</code></pre> <p>\u00c9 importante notar que:</p> <p>Como o MAAS n\u00e3o apareceu na listagem, adicionamos manualmente uma nova cloud por meio de um arquivo de configura\u00e7\u00e3o chamado maas-cloud.yaml, com o seguinte conte\u00fado:</p> <pre><code>clouds:\n  maas-one:\n    type: maas\n    auth-types: [oauth1]\n    endpoint: http://192.168.0.3:5240/MAAS/\n</code></pre> <p>Em seguida, adicionamos a cloud com o comando:</p> <pre><code>$ juju add-cloud --client -f maas-cloud.yaml maas-one\n</code></pre>"},{"location":"roteiro2/main/#24-adicao-de-credenciais-ao-juju","title":"2.4 Adi\u00e7\u00e3o de credenciais ao Juju","text":"<p>Para permitir que o Juju se autenticasse e interagisse com o MAAS, criamos o arquivo maas-creds.yaml, contendo as credenciais de autentica\u00e7\u00e3o:</p> <pre><code>credentials:\n  maas-one:\n    anyuser:\n      auth-type: oauth1\n      maas-oauth: &lt;API KEY&gt;\n</code></pre> <p>O valor  foi substitu\u00eddo pela chave gerada no MAAS, dispon\u00edvel no menu do usu\u00e1rio. <p>As credenciais foram adicionadas com o comando:</p> <pre><code>$ juju add-credential --client -f maas-creds.yaml maas-one\n</code></pre>"},{"location":"roteiro2/main/#25-bootstrap-do-controller-do-juju","title":"2.5 Bootstrap do controller do Juju","text":"<p>Para iniciar o uso do Juju, foi necess\u00e1rio criar um controller, respons\u00e1vel por gerenciar os deploys futuros. Antes do bootstrap, acessamos o dashboard do MAAS e aplicamos a tag juju \u00e0 m\u00e1quina server1.</p> <p>O comando utilizado para o bootstrap foi:</p> <pre><code>$ juju bootstrap --bootstrap-series=jammy --constraints tags=juju maas-one maas-controller\n</code></pre> <p>Esse processo levou alguns minutos, pois envolveu o provisionamento da m\u00e1quina e a instala\u00e7\u00e3o do agente de controle do Juju. Ao final, validamos o sucesso da opera\u00e7\u00e3o com:</p> <pre><code>$ juju status\n</code></pre> <p>O controller foi iniciado com sucesso e passou a ser a interface principal entre os comandos do juju-cli e a infraestrutura gerenciada via MAAS.</p>"},{"location":"roteiro2/main/#3-aplicacao","title":"3. Aplica\u00e7\u00e3o","text":""},{"location":"roteiro2/main/#31-configuracao-do-modelo-para-deploy-das-aplicacoes","title":"3.1 Configura\u00e7\u00e3o do modelo para deploy das aplica\u00e7\u00f5es","text":"<p>Com o controller j\u00e1 inicializado, realizamos a instala\u00e7\u00e3o do Dashboard do Juju - o que permitiu a visualiza\u00e7\u00e3o gr\u00e1fica dos modelos, aplica\u00e7\u00f5es e unidades gerenciadas;</p> <p>Ap\u00f3s esse acesso, retornamos ao terminal e listamos os modelos dispon\u00edveis com:</p> <pre><code>$ juju models\n</code></pre> <p>Em seguida, foi feito o switch para o modelo padr\u00e3o do controller com o comando:</p> <pre><code>$ juju switch maas-controller:admin/maas\n</code></pre> <p>Dessa forma, garantimos que Grafana e Prometheus fossem implantados no modelo correto.</p>"},{"location":"roteiro2/main/#32-preparacao-dos-charms-locais","title":"3.2 Prepara\u00e7\u00e3o dos charms locais","text":"<p>Para realizar o deploy local das aplica\u00e7\u00f5es, foi criada uma pasta dedicada para armazenar os charms baixados do Charmhub:</p> <p><pre><code>$ mkdir -p /home/cloud/charms\n$ cd /home/cloud/charms\n</code></pre> Em seguida, baixamos:</p> <pre><code>$ juju download grafana\n$ juju download prometheus2\n</code></pre>"},{"location":"roteiro2/main/#33-deploy-dos-servicos","title":"3.3 Deploy dos servi\u00e7os","text":"<p>Com a utiliza\u00e7\u00e3o do charm local, fizemos:</p> <p><pre><code>$ juju deploy ./prometheus2_XXX.charm\n$ juju deploy ./grafana_XXX.charm\n</code></pre> Note que:</p> <p>Enquanto ocorria o deploy, observamos analisando o status do Juju.</p>"},{"location":"roteiro2/main/#34-integracao-validacao-e-visualizacao-dos-servicos","title":"3.4 Integra\u00e7\u00e3o, valida\u00e7\u00e3o e visualiza\u00e7\u00e3o dos servi\u00e7os","text":"<p>Com os servi\u00e7os Grafana e Prometheus em estado active, realizamos a integra\u00e7\u00e3o entre eles utilizando os procedimentos descritos no README do charm do Grafana. A integra\u00e7\u00e3o foi feita por meio da interface gr\u00e1fica do Grafana, acessando o painel, criando um novo dashboard e configurando o Prometheus como fonte de dados (source).</p> <p>Na sequ\u00eancia, validamos o funcionamento da aplica\u00e7\u00e3o acessando o dashboard do Grafana a partir da rede do Insper. A conex\u00e3o foi estabelecida com sucesso, comprovando a disponibilidade da aplica\u00e7\u00e3o fora do ambiente local.</p> <p>Por fim, acessamos a interface gr\u00e1fica do Juju, visualisando as aplica\u00e7\u00f5es implantadas e em execu\u00e7\u00e3o dentro do modelo ativo.</p>"},{"location":"roteiro2/main/#tarefa","title":"Tarefa","text":""},{"location":"roteiro2/main/#1-visualizacao-das-maquinas-no-dashboard-do-maas","title":"1. Visualiza\u00e7\u00e3o das m\u00e1quinas no Dashboard do MAAS","text":"<p>A imagem abaixo mostra o painel do MAAS com as cinco m\u00e1quinas cadastradas, onde duas est\u00e3o com status Ready (prontas para uso) e tr\u00eas est\u00e3o em estado Deployed, indicando que est\u00e3o em opera\u00e7\u00e3o. \u00c9 poss\u00edvel visualizar tamb\u00e9m os IPs atribu\u00eddos a cada m\u00e1quina ativa.</p> <p></p>"},{"location":"roteiro2/main/#2-verificacao-do-estado-dos-servicos-com-juju-status","title":"2. Verifica\u00e7\u00e3o do estado dos servi\u00e7os com juju status","text":"<p>O comando juju status foi executado ap\u00f3s o deploy completo das aplica\u00e7\u00f5es. A imagem a seguir mostra que tanto o Grafana quanto o Prometheus est\u00e3o no estado active, indicando que os servi\u00e7os est\u00e3o operando corretamente. Tamb\u00e9m \u00e9 poss\u00edvel verificar as portas abertas e os IPs atribu\u00eddos.</p> <p></p>"},{"location":"roteiro2/main/#3-integracao-entre-grafana-e-prometheus-no-dashboard","title":"3. Integra\u00e7\u00e3o entre Grafana e Prometheus no dashboard","text":"<p>Nesta etapa, foi acessado o dashboard do Grafana, onde foi criado um painel que utiliza o Prometheus como fonte de dados (source). A imagem mostra o gr\u00e1fico gerado a partir da consulta realizada no Prometheus.</p> <p></p>"},{"location":"roteiro2/main/#4-acesso-ao-grafana-a-partir-da-rede-do-insper","title":"4. Acesso ao Grafana a partir da rede do Insper","text":"<p>A pr\u00f3xima imagem comprova o acesso ao dashboard do Grafana a partir da rede do Insper, como evidenciado pela conex\u00e3o ativa \u00e0 rede \"Insper_Alunos\". Isso demonstra que a aplica\u00e7\u00e3o est\u00e1 acess\u00edvel fora da rede local do KIT.</p> <p></p>"},{"location":"roteiro2/main/#5-visualizacao-das-aplicacoes-em-execucao-no-dashboard-do-juju","title":"5. Visualiza\u00e7\u00e3o das aplica\u00e7\u00f5es em execu\u00e7\u00e3o no Dashboard do Juju","text":"<p>Por fim, acessamos a interface gr\u00e1fica do Juju para visualizar o modelo em uso e confirmar que as aplica\u00e7\u00f5es est\u00e3o sendo gerenciadas corretamente. A imagem mostra o Grafana e o Prometheus com status Running, ambos implantados localmente.</p> <p></p>"}]}